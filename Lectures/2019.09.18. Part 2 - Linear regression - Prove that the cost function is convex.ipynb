{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Linear regression: Prove that the cost function is convex</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "you can Find me on Github:\n",
    "> ###### [ GitHub](https://github.com/lev1khachatryan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For supervised learning, models are optimized by finding optimal coefficients that minimize cost function. Cost function is the sum of losses from each data point calculated with loss function. The model we choose to use is our hypothesis. Here is the hypothesis for Linear Regression model.\n",
    "\n",
    "<img src='assets/20190918_Part2/1.png'>\n",
    "\n",
    "The most commonly used loss function for Linear Regression is Least Squared Error, and its cost function is also known as Mean Squared Error(MSE).\n",
    "\n",
    "<img src='assets/20190918_Part2/2.png'>\n",
    "\n",
    "As we can see from the formula, cost function is a parabola curve. To minimize it, we need to find its vertex. It can be solved analytically or by using programming algorithms. In this notebook, I will focus on one of the most popular programming solutions, Gradient Descent, to walk through the optimization process. Gradient Descent is pervasively applied in different models that takes steps based on a learning rate $\\alpha$, moving towards the vertex of the parabola to find the global minimum for Linear Regression, which is also known as the point with the slope equals to 0. So to get the slope, we take the derivative of cost function at each coefficient $\\theta$.\n",
    "\n",
    "<img src='assets/20190918_Part2/3.png'>\n",
    "\n",
    "<img src='assets/20190918_Part2/4.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, continuously update each $\\theta$ with a good amount of iterations along with the observation of reduction in cost function until it reaches a horizontal line. In reality, it is impossible for the slope to reach an exact value of 0 due to the chosen step size, but it can be close to 0 as much as possible depending on hyperparameters.\n",
    "\n",
    "<img src='assets/20190918_Part2/5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New coefficients will be the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/20190918_Part2/6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

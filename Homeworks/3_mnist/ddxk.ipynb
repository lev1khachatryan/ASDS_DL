{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras.preprocessing.image\n",
    "# import sklearn.preprocessing\n",
    "import os;\n",
    "import datetime  \n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from data_loader import *\n",
    "from abc import abstractmethod\n",
    "from utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, train_images_dir, val_images_dir, test_images_dir, train_batch_size, val_batch_size, \n",
    "            test_batch_size, height_of_image, width_of_image, num_channels, num_classes):\n",
    "\n",
    "        self.train_paths = glob.glob(os.path.join(train_images_dir, \"**/*.png\"), recursive=True)\n",
    "        self.val_paths = glob.glob(os.path.join(val_images_dir, \"**/*.png\"), recursive=True)\n",
    "        self.test_paths = glob.glob(os.path.join(test_images_dir, \"**/*.png\"), recursive=True)\n",
    "\n",
    "        random.shuffle(self.train_paths)\n",
    "        random.shuffle(self.val_paths)\n",
    "        random.shuffle(self.test_paths)\n",
    "\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "\n",
    "        self.height_of_image = height_of_image\n",
    "        self.width_of_image = width_of_image\n",
    "        self.num_channels = num_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def load_image(self, path, is_flattened = False):\n",
    "        im = np.asarray(Image.open(path))\n",
    "        lbl = np.eye(self.num_classes)[int(path.rsplit('\\\\', 2)[-2])]\n",
    "\n",
    "        if is_flattened:\n",
    "            im = im.reshape(self.height_of_image * self.width_of_image)\n",
    "\n",
    "        return im, lbl\n",
    "\n",
    "    def batch_data_loader(self, batch_size, file_paths, index, is_flattened = False):\n",
    "        ims = []\n",
    "        lbls = []\n",
    "        \n",
    "        while batch_size >= 1 and (len(file_paths) - index > 0):\n",
    "            im, lbl = self.load_image(file_paths[index], is_flattened)\n",
    "            ims.append(im)\n",
    "            lbls.append(lbl)\n",
    "            batch_size -= 1\n",
    "            index += 1\n",
    "        \n",
    "        return np.array(ims), np.array(lbls)\n",
    "\n",
    "    def train_data_loader(self, index):\n",
    "        return self.batch_data_loader(self.train_batch_size, self.train_paths, index)\n",
    "\n",
    "    def val_data_loader(self, index):\n",
    "        return self.batch_data_loader(self.val_batch_size, self.val_paths, index)\n",
    "\n",
    "    def test_data_loader(self, index):\n",
    "        return self.batch_data_loader(self.test_batch_size, self.test_paths, index)\n",
    "    \n",
    "    def get_train_data_size(self):\n",
    "        return len(self.train_paths)\n",
    "    \n",
    "    def get_val_data_size(self):\n",
    "        return len(self.val_paths)\n",
    "    \n",
    "    def get_test_data_size(self):\n",
    "        return len(self.test_paths)\n",
    "    \n",
    "    def all_train_data_loader(self, is_flattened = False):\n",
    "        return self.batch_data_loader(self.get_train_data_size(), self.train_paths, 0)\n",
    "    \n",
    "    def all_val_data_loader(self, is_flattened = False):\n",
    "        return self.batch_data_loader(self.get_val_data_size(), self.val_paths, 0)\n",
    "    \n",
    "    def all_test_data_loader(self, is_flattened = False):\n",
    "        return self.batch_data_loader(self.get_test_data_size(), self.test_paths, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from data_loader import *\n",
    "from utils import *\n",
    "from abc import abstractmethod\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os;\n",
    "import datetime  \n",
    "import cv2\n",
    "\n",
    "class BaseNN:\n",
    "    def __init__(self, train_images_dir, val_images_dir, test_images_dir, num_epochs, train_batch_size,\n",
    "                 val_batch_size, test_batch_size, height_of_image, width_of_image, num_channels, \n",
    "                 num_classes, learning_rate, base_dir, max_to_keep, model_name, keep_prob):\n",
    "\n",
    "        self.data_loader = DataLoader(train_images_dir, val_images_dir, test_images_dir, train_batch_size, \n",
    "                val_batch_size, test_batch_size, height_of_image, width_of_image, num_channels, num_classes)\n",
    "\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.base_dir = base_dir\n",
    "        self.max_to_keep = max_to_keep\n",
    "        self.model_name = model_name\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        ####\n",
    "        self.index_in_epoch = 0\n",
    "        self.current_epoch = 0\n",
    "        self.n_log_step = 0 # counting current number of mini batches trained on\n",
    "\n",
    "        # permutation array\n",
    "        self.perm_array = np.array([])\n",
    "        ####\n",
    "        \n",
    "    # function to get the next mini batch\n",
    "    def next_mini_batch(self, mb_size):\n",
    "        start = self.index_in_epoch\n",
    "        self.index_in_epoch += mb_size\n",
    "        self.current_epoch += mb_size/len(self.x_train)  \n",
    "        \n",
    "        # adapt length of permutation array\n",
    "        if not len(self.perm_array) == len(self.x_train):\n",
    "            self.perm_array = np.arange(len(self.x_train))\n",
    "        \n",
    "        # shuffle once at the start of epoch\n",
    "        if start == 0:\n",
    "            np.random.shuffle(self.perm_array)\n",
    "\n",
    "        # at the end of the epoch\n",
    "        if self.index_in_epoch > self.x_train.shape[0]:\n",
    "            np.random.shuffle(self.perm_array) # shuffle data\n",
    "            start = 0 # start next epoch\n",
    "            self.index_in_epoch = mb_size # set index to mini batch size\n",
    "                \n",
    "        end = self.index_in_epoch\n",
    "\n",
    "        x_tr = self.x_train[self.perm_array[start:end]]\n",
    "        y_tr = self.y_train[self.perm_array[start:end]]\n",
    "\n",
    "        return x_tr, y_tr\n",
    "    \n",
    "    def create_network(self):\n",
    "        \"\"\"\n",
    "        Create base components of the network.\n",
    "        Main structure of network will be described in network function.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            None\n",
    "        Returns:\n",
    "            None\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # variables for input and output \n",
    "        self.x_data_tf = tf.placeholder(dtype=tf.float32, shape=[None, self.data_loader.height_of_image, self.data_loader.width_of_image, self.data_loader.num_channels], name='x_data_tf')\n",
    "        self.y_data_tf = tf.placeholder(dtype=tf.float32, shape=[None, self.data_loader.num_classes], name='y_data_tf')\n",
    "\n",
    "        self.z_pred_tf = self.network(self.x_data_tf)\n",
    "\n",
    "        # cost function\n",
    "        self.cross_entropy_tf = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=self.y_data_tf, logits=self.z_pred_tf), name = 'cross_entropy_tf')\n",
    "\n",
    "        # optimisation function\n",
    "        self.learn_rate_tf = tf.placeholder(dtype=tf.float32, name=\"learn_rate_tf\")\n",
    "        self.train_step_tf = tf.train.AdamOptimizer(self.learn_rate_tf).minimize(\n",
    "            self.cross_entropy_tf, name = 'train_step_tf')\n",
    "\n",
    "        # predicted probabilities in one-hot encoding\n",
    "        self.y_pred_proba_tf = tf.nn.softmax(self.z_pred_tf, name='y_pred_proba_tf') \n",
    "        \n",
    "        # tensor of correct predictions\n",
    "        self.y_pred_correct_tf = tf.equal(tf.argmax(self.y_pred_proba_tf, 1),\n",
    "                                          tf.argmax(self.y_data_tf, 1),\n",
    "                                          name = 'y_pred_correct_tf')  \n",
    "        \n",
    "        # accuracy \n",
    "        self.accuracy_tf = tf.reduce_mean(tf.cast(self.y_pred_correct_tf, dtype=tf.float32),\n",
    "                                         name = 'accuracy_tf')\n",
    "\n",
    "        # tensors to save intermediate accuracies and losses during training\n",
    "        self.train_loss_tf = tf.Variable(np.array([]), dtype=tf.float32, \n",
    "                                         name='train_loss_tf', validate_shape = False)\n",
    "        self.valid_loss_tf = tf.Variable(np.array([]), dtype=tf.float32,\n",
    "                                         name='valid_loss_tf', validate_shape = False)\n",
    "        self.train_acc_tf = tf.Variable(np.array([]), dtype=tf.float32, \n",
    "                                        name='train_acc_tf', validate_shape = False)\n",
    "        self.valid_acc_tf = tf.Variable(np.array([]), dtype=tf.float32, \n",
    "                                        name='valid_acc_tf', validate_shape = False)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def summary_variable(self, var, var_name):\n",
    "        \"\"\"\n",
    "        Attach summaries to a tensor for TensorBoard visualization\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            var         - variable we want to attach\n",
    "            var_name    - name of the variable\n",
    "        Returns:\n",
    "            None\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        with tf.name_scope(var_name):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "        return None\n",
    "    \n",
    "    def attach_summary(self, sess):\n",
    "        \"\"\"\n",
    "        Create summary tensors for tensorboard.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            sess - the session for which we want to create summaries\n",
    "        Returns:\n",
    "            None\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        self.summary_variable(self.W_conv1_tf, 'W_conv1_tf')\n",
    "        self.summary_variable(self.b_conv1_tf, 'b_conv1_tf')\n",
    "        self.summary_variable(self.W_conv2_tf, 'W_conv2_tf')\n",
    "        self.summary_variable(self.b_conv2_tf, 'b_conv2_tf')\n",
    "        self.summary_variable(self.W_conv3_tf, 'W_conv3_tf')\n",
    "        self.summary_variable(self.b_conv3_tf, 'b_conv3_tf')\n",
    "        self.summary_variable(self.W_fc1_tf, 'W_fc1_tf')\n",
    "        self.summary_variable(self.b_fc1_tf, 'b_fc1_tf')\n",
    "        self.summary_variable(self.W_fc2_tf, 'W_fc2_tf')\n",
    "        self.summary_variable(self.b_fc2_tf, 'b_fc2_tf')\n",
    "        tf.summary.scalar('cross_entropy_tf', self.cross_entropy_tf)\n",
    "        tf.summary.scalar('accuracy_tf', self.accuracy_tf)\n",
    "\n",
    "        # merge all summaries for tensorboard\n",
    "        self.merged = tf.summary.merge_all()\n",
    "\n",
    "        # initialize summary writer \n",
    "        timestamp = datetime.datetime.now().strftime('%d-%m-%Y_%H-%M-%S')\n",
    "        filepath = os.path.join(os.getcwd(), self.base_dir, self.model_name, 'logs', (self.model_name+'_'+timestamp))\n",
    "        self.train_writer = tf.summary.FileWriter(os.path.join(filepath,'train'), sess.graph)\n",
    "        self.valid_writer = tf.summary.FileWriter(os.path.join(filepath,'valid'), sess.graph)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def close_writers(self):\n",
    "        \"\"\"\n",
    "        Close train and validation summary writer.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            sess - the session we want to save\n",
    "        Returns:\n",
    "            None\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        self.train_writer.close()\n",
    "        self.valid_writer.close()\n",
    "\n",
    "        return None\n",
    "\n",
    "    def save_model(self, sess):\n",
    "        \"\"\"\n",
    "        Save tensors/summaries\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            sess - the session we want to save\n",
    "        Returns:\n",
    "            None\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(os.getcwd(), self.base_dir, self.model_name, 'checkpoints', self.model_name)\n",
    "        self.saver_tf.save(sess, filepath)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def train_model_helper(self, sess, x_train, y_train, x_valid, y_valid, n_epoch = 1):        \n",
    "        \"\"\"\n",
    "        Helper function to train the model.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            sess - the session for which we want to create summaries\n",
    "            x_train (matrix_like) - train images\n",
    "            y_train (matrix_like) - labels of train images\n",
    "            x_valid (matrix_like) - validation images\n",
    "            y_valid (matrix_like) - labels of validation images\n",
    "            n_epoch (int)         - number of epochs\n",
    "        Returns:\n",
    "            None\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        \n",
    "        # parameters\n",
    "        mb_per_epoch = self.x_train.shape[0]/self.data_loader.train_batch_size\n",
    "        train_loss, train_acc, valid_loss, valid_acc = [],[],[],[]\n",
    "        \n",
    "        # start timer\n",
    "        start = datetime.datetime.now();\n",
    "        print(datetime.datetime.now().strftime('%d-%m-%Y %H:%M:%S'),': start training')\n",
    "        print('learnrate = ', self.learning_rate,', n_epoch = ', n_epoch,\n",
    "              ', mb_size = ', self.data_loader.train_batch_size)\n",
    "        # looping over mini batches\n",
    "        for i in range(int(n_epoch*mb_per_epoch)+1):            \n",
    "            # get new batch\n",
    "            x_batch, y_batch = self.next_mini_batch(self.data_loader.train_batch_size)\n",
    "\n",
    "            # run the graph\n",
    "            self.sess.run(self.train_step_tf, feed_dict={self.x_data_tf: x_batch, \n",
    "                                                    self.y_data_tf: y_batch, \n",
    "                                                    self.keep_prob_tf: self.keep_prob, \n",
    "                                                    self.learn_rate_tf: self.learning_rate})\n",
    "            \n",
    "            feed_dict_valid = {self.x_data_tf: self.x_valid, \n",
    "                               self.y_data_tf: self.y_valid, \n",
    "                               self.keep_prob_tf: 1.0}\n",
    "            # feed_dict_train = {self.x_data_tf: self.x_train[self.perm_array[:len(self.x_valid)]], \n",
    "            #                     self.y_data_tf: self.y_train[self.perm_array[:len(self.y_valid)]], \n",
    "            #                     self.keep_prob_tf: 1.0}\n",
    "            feed_dict_train = {self.x_data_tf: x_batch, \n",
    "                                self.y_data_tf: y_batch, \n",
    "                                self.keep_prob_tf: 1.0}\n",
    "            \n",
    "            # store losses and accuracies\n",
    "            if i%self.validation_step == 0:\n",
    "                valid_loss.append(sess.run(self.cross_entropy_tf,\n",
    "                                           feed_dict = feed_dict_valid))\n",
    "                valid_acc.append(self.accuracy_tf.eval(session = sess, \n",
    "                                                       feed_dict = feed_dict_valid))\n",
    "                print('%.2f epoch, %.2f iteration: val loss = %.4f, val acc = %.4f'%(\n",
    "                    self.current_epoch, i, valid_loss[-1],valid_acc[-1]))\n",
    "\n",
    "            # summary for tensorboard\n",
    "            if i%self.summary_step == 0:\n",
    "                self.n_log_step += 1 # for logging the results\n",
    "                train_summary = sess.run(self.merged, feed_dict={self.x_data_tf: x_batch, \n",
    "                                                                self.y_data_tf: y_batch, \n",
    "                                                                self.keep_prob_tf: 1.0})\n",
    "                valid_summary = sess.run(self.merged, feed_dict = feed_dict_valid)\n",
    "                self.train_writer.add_summary(train_summary, self.n_log_step)\n",
    "                self.valid_writer.add_summary(valid_summary, self.n_log_step)\n",
    "                \n",
    "            if i%self.display_step == 0:\n",
    "                train_loss.append(sess.run(self.cross_entropy_tf,\n",
    "                                           feed_dict = feed_dict_train))\n",
    "                train_acc.append(self.accuracy_tf.eval(session = sess, \n",
    "                                                       feed_dict = feed_dict_train))\n",
    "                print('%.2f epoch, %.2f iteration: train loss = %.4f, train acc = %.4f'%(\n",
    "                    self.current_epoch, i,  train_loss[-1],train_acc[-1]))\n",
    "                \n",
    "            # save current model to disk\n",
    "            if i%self.checkpoint_step == 0:\n",
    "                self.save_model(sess)\n",
    "                \n",
    "        # concatenate losses and accuracies and assign to tensor variables\n",
    "        tl_c = np.concatenate([self.train_loss_tf.eval(session=sess), train_loss], axis = 0)\n",
    "        vl_c = np.concatenate([self.valid_loss_tf.eval(session=sess), valid_loss], axis = 0)\n",
    "        ta_c = np.concatenate([self.train_acc_tf.eval(session=sess), train_acc], axis = 0)\n",
    "        va_c = np.concatenate([self.valid_acc_tf.eval(session=sess), valid_acc], axis = 0)\n",
    "   \n",
    "        sess.run(tf.assign(self.train_loss_tf, tl_c, validate_shape = False))\n",
    "        sess.run(tf.assign(self.valid_loss_tf, vl_c , validate_shape = False))\n",
    "        sess.run(tf.assign(self.train_acc_tf, ta_c , validate_shape = False))\n",
    "        sess.run(tf.assign(self.valid_acc_tf, va_c , validate_shape = False))\n",
    "        \n",
    "        print('running time for training: ', datetime.datetime.now() - start)\n",
    "        return None\n",
    "    \n",
    "    def train_model(self, display_step, validation_step, checkpoint_step, summary_step):\n",
    "        \"\"\"\n",
    "        Main function for model training.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            display_step       (int)   - Number of steps we cycle through before displaying detailed progress\n",
    "            validation_step    (int)   - Number of steps we cycle through before validating the model\n",
    "            checkpoint_step    (int)   - Number of steps we cycle through before saving checkpoint\n",
    "            summary_step       (int)   - Number of steps we cycle through before saving summary\n",
    "        Returns:\n",
    "            None\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        self.display_step = display_step\n",
    "        self.validation_step = validation_step\n",
    "        self.checkpoint_step = checkpoint_step\n",
    "        self.summary_step = summary_step\n",
    "        \n",
    "        # training and validation data\n",
    "        self.x_train, self.y_train = self.data_loader.all_train_data_loader()\n",
    "        self.x_valid, self.y_valid = self.data_loader.all_val_data_loader()\n",
    "\n",
    "        self.x_train = self.x_train.reshape(-1, self.data_loader.height_of_image, self.data_loader.width_of_image, self.data_loader.num_channels)\n",
    "        self.x_valid = self.x_valid.reshape(-1, self.data_loader.height_of_image, self.data_loader.width_of_image, self.data_loader.num_channels)\n",
    "\n",
    "        self.saver_tf = tf.train.Saver(max_to_keep = self.max_to_keep)\n",
    "\n",
    "        # attach summaries\n",
    "        self.attach_summary(self.sess)\n",
    "\n",
    "        # variable initialization of the default graph\n",
    "        self.sess.run(tf.global_variables_initializer()) \n",
    "\n",
    "        # training on original data\n",
    "        self.train_model_helper(self.sess, self.x_train, self.y_train, self.x_valid, self.y_valid, n_epoch = self.num_epochs)\n",
    "\n",
    "        # save final model\n",
    "        self.save_model(self.sess)\n",
    "\n",
    "        self.close_writers()\n",
    "\n",
    "    def get_accuracy(self, sess):\n",
    "        \"\"\"\n",
    "        Get accuracies of training and validation sets.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            sess - session\n",
    "        Returns:\n",
    "            tuple (tuple of lists) train and validation accuracies\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        train_acc = self.train_acc_tf.eval(session = sess)\n",
    "        valid_acc = self.valid_acc_tf.eval(session = sess)\n",
    "        return train_acc, valid_acc\n",
    "\n",
    "    def get_loss(self, sess):\n",
    "        \"\"\"\n",
    "        Get losses of training and validation sets.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            sess - session\n",
    "        Returns:\n",
    "            tuple (tuple of lists) train and validation losses\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        train_loss = self.train_loss_tf.eval(session = sess)\n",
    "        valid_loss = self.valid_loss_tf.eval(session = sess)\n",
    "        return train_loss, valid_loss \n",
    "\n",
    "    def forward(self, sess, x_data):\n",
    "        \"\"\"\n",
    "        Forward prediction of current graph.\n",
    "        Will be used in test_model method.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            sess                 - actual session\n",
    "            x_data (matrix_like) - data for which we want to calculate predicted probabilities\n",
    "        Returns:\n",
    "            vector_like - predicted probabilities for input data\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        y_pred_proba = self.y_pred_proba_tf.eval(session = sess, \n",
    "                                                 feed_dict = {self.x_data_tf: x_data,\n",
    "                                                              self.keep_prob_tf: 1.0})\n",
    "        return y_pred_proba\n",
    "    \n",
    "    def load_session_from_file(self, filename):\n",
    "        \"\"\"\n",
    "        Load session from file, restore graph, and load tensors.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            filename (string) - the name of the model (name of file we saved in disk)\n",
    "        Returns:\n",
    "            session\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # filepath = os.path.join(os.getcwd(), filename + '.meta')\n",
    "        filepath = os.path.join(os.getcwd(), self.base_dir, self.model_name, 'checkpoints', filename + '.meta')\n",
    "        \n",
    "        print('Load model from directory: ', filepath)\n",
    "\n",
    "        saver = tf.train.import_meta_graph(filepath)\n",
    "        sess = tf.Session()\n",
    "#         saver.restore(sess, self.model_name)\n",
    "#         graph = tf.get_default_graph()\n",
    "        \n",
    "#         self.load_tensors(graph)\n",
    "        \n",
    "        return sess\n",
    "\n",
    "    def test_model(self):\n",
    "        \"\"\"\n",
    "        load model and test on test data.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            None\n",
    "        Returns:\n",
    "            metric, defined in dnn class (for example accuracy)\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        x_test, y_test = self.data_loader.all_test_data_loader()\n",
    "        x_test = x_test.reshape(-1, self.data_loader.height_of_image, self.data_loader.width_of_image, self.data_loader.num_channels)\n",
    "        \n",
    "        \n",
    "        sess = self.load_session_from_file(self.model_name)\n",
    "        \n",
    "        y_test_pred = {}\n",
    "        y_test_pred_labels = {}\n",
    "        y_test_pred[self.model_name] = self.forward(sess, x_test)\n",
    "\n",
    "        sess.close()\n",
    "        \n",
    "        y_test_pred_labels[self.model_name] = one_hot_to_dense(y_test_pred[self.model_name])\n",
    "        y_test = one_hot_to_dense(y_test)\n",
    "        \n",
    "        print('Test Accuracy: ', self.metrics(y_test, y_test_pred_labels[self.model_name]))\n",
    "        return self.metrics(y_test, y_test_pred_labels[self.model_name])\n",
    "        \n",
    "    # Initialize network from meta file\n",
    "    # def initialize_network(self):\n",
    "    # \tfilepath = os.path.join(os.getcwd(), self.model_name + '.meta')\n",
    "    # \tif os.path.isdir(filepath):\n",
    "    # \t\tself.load_session_from_file(self.model_name)\n",
    "    # \treturn None\n",
    "\n",
    "    def initialize_network(self):\n",
    "        \"\"\"\n",
    "        Initialize network from last checkpoint if exists, otherwise initialize with random values.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            None\n",
    "        Returns:\n",
    "            metric, defined in dnn class (for example accuracy)\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        filepath = os.path.join(os.getcwd(), self.base_dir, self.model_name, 'checkpoints', self.model_name + '.meta')\n",
    "        if ~os.path.isdir(filepath):\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "        else:\n",
    "            self.sess = self.load_session_from_file(self.model_name)\n",
    "        return None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def network(self, X):\n",
    "        raise NotImplementedError('subclasses must override network()!')\n",
    "\n",
    "    @abstractmethod\n",
    "    def metrics(self, Y, y_pred):\n",
    "        raise NotImplementedError('subclasses must override metrics()!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(BaseNN):\n",
    "\n",
    "    def __init__(self, train_images_dir, val_images_dir, test_images_dir, num_epochs, train_batch_size,\n",
    "                 val_batch_size, test_batch_size, height_of_image, width_of_image, num_channels, \n",
    "                 num_classes, learning_rate, base_dir, max_to_keep, model_name, keep_prob):\n",
    "\n",
    "        super().__init__(train_images_dir, val_images_dir, test_images_dir, num_epochs, train_batch_size,\n",
    "                 val_batch_size, test_batch_size, height_of_image, width_of_image, num_channels, \n",
    "                 num_classes, learning_rate, base_dir, max_to_keep, model_name, keep_prob)\n",
    "\n",
    "        self.s_f_conv1 = 3; # filter size of first convolution layer (default = 3)\n",
    "        self.n_f_conv1 = 36; # number of features of first convolution layer (default = 36)\n",
    "        self.s_f_conv2 = 3; # filter size of second convolution layer (default = 3)\n",
    "        self.n_f_conv2 = 36; # number of features of second convolution layer (default = 36)\n",
    "        self.s_f_conv3 = 3; # filter size of third convolution layer (default = 3)\n",
    "        self.n_f_conv3 = 36; # number of features of third convolution layer (default = 36)\n",
    "        self.n_n_fc1 = 576; # number of neurons of first fully connected layer (default = 576)\n",
    "\n",
    "    def weight_variable(self, shape, name = None):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            shape   (tuple)     - shape of weight variable\n",
    "            name    (string)    - name of weight variable\n",
    "        Returns:\n",
    "            tf.Variable         - initialized weight variable\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial, name = name)\n",
    "\n",
    "    def bias_variable(self, shape, name = None):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            shape   (tuple)     - shape of bias variable\n",
    "            name    (string)    - name of bias variable\n",
    "        Returns:\n",
    "            tf.Variable         - initialized bias variable\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        initial = tf.constant(0.1, shape=shape) #  positive bias\n",
    "        return tf.Variable(initial, name = name)\n",
    "\n",
    "    def conv2d(self, x, W, name = None):\n",
    "        \"\"\"\n",
    "        2D convolution\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            x       (matrix-like)   - input data\n",
    "            W       (tf.Variable)   - Weight matrix\n",
    "            name    (string)        - name of the graph node\n",
    "        Returns:\n",
    "            tf.Conv                 - Convolution response\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name = name)\n",
    "\n",
    "    def max_pool_2x2(self, x, name = None):\n",
    "        \"\"\"\n",
    "        2 x 2 max pooling\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            x       (matrix-like)   - matrix we want to apply max pooling\n",
    "            name    (string)        - name of the graph node\n",
    "        Returns:\n",
    "            tf.max_pool             - max pooling response\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name = name)\n",
    "    \n",
    "    def load_tensors(self, graph):\n",
    "        \"\"\"\n",
    "        load tensors from a saved graph\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            graph       (tf.graph_like) - graph we obtained from saved file\n",
    "        Returns:\n",
    "            None\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "\n",
    "        # input tensors\n",
    "        self.x_data_tf = graph.get_tensor_by_name(\"x_data_tf:0\")\n",
    "        self.y_data_tf = graph.get_tensor_by_name(\"y_data_tf:0\")\n",
    "        \n",
    "        # weights and bias tensors\n",
    "        self.W_conv1_tf = graph.get_tensor_by_name(\"W_conv1_tf:0\")\n",
    "        self.W_conv2_tf = graph.get_tensor_by_name(\"W_conv2_tf:0\")\n",
    "        self.W_conv3_tf = graph.get_tensor_by_name(\"W_conv3_tf:0\")\n",
    "        self.W_fc1_tf = graph.get_tensor_by_name(\"W_fc1_tf:0\")\n",
    "        self.W_fc2_tf = graph.get_tensor_by_name(\"W_fc2_tf:0\")\n",
    "        self.b_conv1_tf = graph.get_tensor_by_name(\"b_conv1_tf:0\")\n",
    "        self.b_conv2_tf = graph.get_tensor_by_name(\"b_conv2_tf:0\")\n",
    "        self.b_conv3_tf = graph.get_tensor_by_name(\"b_conv3_tf:0\")\n",
    "        self.b_fc1_tf = graph.get_tensor_by_name(\"b_fc1_tf:0\")\n",
    "        self.b_fc2_tf = graph.get_tensor_by_name(\"b_fc2_tf:0\")\n",
    "        \n",
    "        # activation tensors\n",
    "        self.h_conv1_tf = graph.get_tensor_by_name('h_conv1_tf:0')  \n",
    "        self.h_pool1_tf = graph.get_tensor_by_name('h_pool1_tf:0')\n",
    "        self.h_conv2_tf = graph.get_tensor_by_name('h_conv2_tf:0')\n",
    "        self.h_pool2_tf = graph.get_tensor_by_name('h_pool2_tf:0')\n",
    "        self.h_conv3_tf = graph.get_tensor_by_name('h_conv3_tf:0')\n",
    "        self.h_pool3_tf = graph.get_tensor_by_name('h_pool3_tf:0')\n",
    "        self.h_fc1_tf = graph.get_tensor_by_name('h_fc1_tf:0')\n",
    "        self.z_pred_tf = graph.get_tensor_by_name('z_pred_tf:0')\n",
    "        \n",
    "        # training and prediction tensors\n",
    "        self.learn_rate_tf = graph.get_tensor_by_name(\"learn_rate_tf:0\")\n",
    "        self.keep_prob_tf = graph.get_tensor_by_name(\"keep_prob_tf:0\")\n",
    "        self.cross_entropy_tf = graph.get_tensor_by_name('cross_entropy_tf:0')\n",
    "        self.train_step_tf = graph.get_operation_by_name('train_step_tf')\n",
    "        self.z_pred_tf = graph.get_tensor_by_name('z_pred_tf:0')\n",
    "        self.y_pred_proba_tf = graph.get_tensor_by_name(\"y_pred_proba_tf:0\")\n",
    "        self.y_pred_correct_tf = graph.get_tensor_by_name('y_pred_correct_tf:0')\n",
    "        self.accuracy_tf = graph.get_tensor_by_name('accuracy_tf:0')\n",
    "        \n",
    "        # tensor of stored losses and accuracies during training\n",
    "        self.train_loss_tf = graph.get_tensor_by_name(\"train_loss_tf:0\")\n",
    "        self.train_acc_tf = graph.get_tensor_by_name(\"train_acc_tf:0\")\n",
    "        self.valid_loss_tf = graph.get_tensor_by_name(\"valid_loss_tf:0\")\n",
    "        self.valid_acc_tf = graph.get_tensor_by_name(\"valid_acc_tf:0\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    def network(self, X):\n",
    "        \"\"\"\n",
    "        Construct network architecture\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            X       (tensor) - input data with shape of (batch size; height of image; width of image ; num channels)\n",
    "        Returns:\n",
    "            Last layer of the network (for continuation)\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "\n",
    "        # 1.layer: convolution + max pooling\n",
    "        self.W_conv1_tf = self.weight_variable([self.s_f_conv1, self.s_f_conv1, 1, self.n_f_conv1], name = 'W_conv1_tf') # (3,3,1,36)\n",
    "        self.b_conv1_tf = self.bias_variable([self.n_f_conv1], name = 'b_conv1_tf') # (36)\n",
    "        self.h_conv1_tf = tf.nn.relu(self.conv2d(X, self.W_conv1_tf) + self.b_conv1_tf, name = 'h_conv1_tf') # (.,28,28,36)\n",
    "        self.h_pool1_tf = self.max_pool_2x2(self.h_conv1_tf, name = 'h_pool1_tf') # (.,14,14,36)\n",
    "\n",
    "        # 2.layer: convolution + max pooling\n",
    "        self.W_conv2_tf = self.weight_variable([self.s_f_conv2, self.s_f_conv2, self.n_f_conv1, self.n_f_conv2], name = 'W_conv2_tf')\n",
    "        self.b_conv2_tf = self.bias_variable([self.n_f_conv2], name = 'b_conv2_tf')\n",
    "        self.h_conv2_tf = tf.nn.relu(self.conv2d(self.h_pool1_tf, self.W_conv2_tf) + self.b_conv2_tf, name ='h_conv2_tf') #(.,14,14,36)\n",
    "        self.h_pool2_tf = self.max_pool_2x2(self.h_conv2_tf, name = 'h_pool2_tf') #(.,7,7,36)\n",
    "\n",
    "        # 3.layer: convolution + max pooling\n",
    "        self.W_conv3_tf = self.weight_variable([self.s_f_conv3, self.s_f_conv3, self.n_f_conv2, self.n_f_conv3], name = 'W_conv3_tf')\n",
    "        self.b_conv3_tf = self.bias_variable([self.n_f_conv3], name = 'b_conv3_tf')\n",
    "        self.h_conv3_tf = tf.nn.relu(self.conv2d(self.h_pool2_tf, self.W_conv3_tf) + self.b_conv3_tf, name = 'h_conv3_tf') #(.,7,7,36)\n",
    "        self.h_pool3_tf = self.max_pool_2x2(self.h_conv3_tf, name = 'h_pool3_tf') # (.,4,4,36)\n",
    "\n",
    "        # 4.layer: fully connected\n",
    "        self.W_fc1_tf = self.weight_variable([4*4*self.n_f_conv3,self.n_n_fc1], name = 'W_fc1_tf') # (4*4*36, 576)\n",
    "        self.b_fc1_tf = self.bias_variable([self.n_n_fc1], name = 'b_fc1_tf') # (576)\n",
    "        self.h_pool3_flat_tf = tf.reshape(self.h_pool3_tf, [-1,4*4*self.n_f_conv3], name = 'h_pool3_flat_tf') # (.,576)\n",
    "        self.h_fc1_tf = tf.nn.relu(tf.matmul(self.h_pool3_flat_tf, self.W_fc1_tf) + self.b_fc1_tf, name = 'h_fc1_tf') # (.,576)\n",
    "      \n",
    "        # add dropout\n",
    "        self.keep_prob_tf = tf.placeholder(dtype=tf.float32, name = 'keep_prob_tf')\n",
    "        self.h_fc1_drop_tf = tf.nn.dropout(self.h_fc1_tf, self.keep_prob_tf, name = 'h_fc1_drop_tf')\n",
    "\n",
    "        # 5.layer: fully connected\n",
    "        self.W_fc2_tf = self.weight_variable([self.n_n_fc1, 10], name = 'W_fc2_tf')\n",
    "        self.b_fc2_tf = self.bias_variable([10], name = 'b_fc2_tf')\n",
    "        \n",
    "        self.z_pred_tf = tf.add(tf.matmul(self.h_fc1_drop_tf, self.W_fc2_tf), self.b_fc2_tf, name = 'z_pred_tf')# => (.,10)\n",
    "\n",
    "        return self.z_pred_tf\n",
    "\n",
    "    def metrics(self, Y, Y_pred):\n",
    "        \"\"\"\n",
    "        Some metric, here I use simple accuracy\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            Y       (array_like) - actual labels\n",
    "            Y_pred  (array_like) - predicted labels\n",
    "        Returns:\n",
    "            Float (Accuracy)\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "\n",
    "        Y = Y.reshape(-1,)\n",
    "        Y_pred = Y_pred.reshape(-1,)\n",
    "        return np.mean(Y == Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_graph = DNN(\n",
    "    train_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/train/image/',\n",
    "    val_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/validation/image/',\n",
    "    test_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/test/image/',\n",
    "    num_epochs=1,\n",
    "    train_batch_size=100,\n",
    "    val_batch_size=100,\n",
    "    test_batch_size=100,\n",
    "    height_of_image=28,\n",
    "    width_of_image=28,\n",
    "    num_channels=1,\n",
    "    num_classes=10,\n",
    "    learning_rate = 0.001,\n",
    "    base_dir='results',\n",
    "    max_to_keep=5,\n",
    "    model_name='nn_2',\n",
    "    keep_prob=0.33\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_graph.create_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_graph.initialize_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_step, validation_step, checkpoint_step, summary_step\n",
    "# nn_graph.train_model(100, 100, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model from directory:  C:\\_Files\\MyProjects\\ASDS_3\\ASDS_DL\\Homeworks\\3_mnist\\results\\nn_2\\checkpoints\\nn_2.meta\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-6d84b7759865>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnn_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-46-483035ca1a7b>\u001b[0m in \u001b[0;36mtest_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[0my_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[0my_test_pred_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m         \u001b[0my_test_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-483035ca1a7b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, sess, x_data)\u001b[0m\n\u001b[0;32m    371\u001b[0m         y_pred_proba = self.y_pred_proba_tf.eval(session = sess, \n\u001b[0;32m    372\u001b[0m                                                  feed_dict = {self.x_data_tf: x_data,\n\u001b[1;32m--> 373\u001b[1;33m                                                               self.keep_prob_tf: 1.0})\n\u001b[0m\u001b[0;32m    374\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my_pred_proba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m     \"\"\"\n\u001b[1;32m--> 731\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   5574\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5575\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5576\u001b[1;33m       raise ValueError(\"Cannot use the given session to evaluate tensor: \"\n\u001b[0m\u001b[0;32m   5577\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5578\u001b[0m                        \"graph.\")\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph."
     ]
    }
   ],
   "source": [
    "nn_graph.test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/train/image/'\n",
    "val_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/validation/image/'\n",
    "test_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/test/image/'\n",
    "num_epochs=1\n",
    "train_batch_size=100\n",
    "val_batch_size=100\n",
    "test_batch_size=100\n",
    "height_of_image=28\n",
    "width_of_image=28\n",
    "num_channels=1\n",
    "num_classes=10\n",
    "learning_rate = 0.001\n",
    "base_dir='results'\n",
    "max_to_keep=5\n",
    "model_name='nn_2'\n",
    "keep_prob=0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_session_from_file(filename):\n",
    "    \"\"\"\n",
    "    Load session from file, restore graph, and load tensors.\n",
    "    -----------------\n",
    "    Parameters:\n",
    "        filename (string) - the name of the model (name of file we saved in disk)\n",
    "    Returns:\n",
    "        session\n",
    "    -----------------\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "#     filepath = os.path.join(os.getcwd(), filename + '.meta')\n",
    "#     filepath = os.path.join(os.getcwd(), base_dir, model_name, filename + '.meta')\n",
    "    filepath = os.path.join(os.getcwd(), base_dir, model_name, 'checkpoints_ddxk', filename + '.meta')\n",
    "\n",
    "    print('Load model from directory: ', filepath)\n",
    "\n",
    "    saver = tf.train.import_meta_graph(filepath)\n",
    "    sess = tf.Session()\n",
    "    saver.restore(sess, model_name)\n",
    "    graph = tf.get_default_graph()\n",
    "\n",
    "# #     self.load_tensors(graph)\n",
    "\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\_Files\\\\MyProjects\\\\ASDS_3\\\\ASDS_DL\\\\Homeworks\\\\3_mnist\\\\results\\\\nn_2\\\\checkpoints_ddxk'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = os.path.join(os.getcwd(), base_dir, model_name, 'checkpoints_ddxk')\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_session_from_file(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir(filepath) if isfile(join(filepath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

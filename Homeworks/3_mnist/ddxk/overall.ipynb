{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras.preprocessing.image\n",
    "# import sklearn.preprocessing\n",
    "import os;\n",
    "import datetime  \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from data_loader import *\n",
    "from abc import abstractmethod\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, train_images_dir, val_images_dir, test_images_dir, train_batch_size, val_batch_size, \n",
    "            test_batch_size, height_of_image, width_of_image, num_channels, num_classes):\n",
    "\n",
    "        self.train_paths = glob.glob(os.path.join(train_images_dir, \"**/*.png\"), recursive=True)\n",
    "        self.val_paths = glob.glob(os.path.join(val_images_dir, \"**/*.png\"), recursive=True)\n",
    "        self.test_paths = glob.glob(os.path.join(test_images_dir, \"**/*.png\"), recursive=True)\n",
    "\n",
    "        random.shuffle(self.train_paths)\n",
    "        random.shuffle(self.val_paths)\n",
    "        random.shuffle(self.test_paths)\n",
    "\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "\n",
    "        self.height_of_image = height_of_image\n",
    "        self.width_of_image = width_of_image\n",
    "        self.num_channels = num_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def load_image(self, path, is_flattened = False):\n",
    "        im = np.asarray(Image.open(path))\n",
    "        lbl = np.eye(self.num_classes)[int(path.rsplit('\\\\', 2)[-2])]\n",
    "\n",
    "        if is_flattened:\n",
    "            im = im.reshape(self.height_of_image * self.width_of_image)\n",
    "\n",
    "        return im, lbl\n",
    "\n",
    "    def batch_data_loader(self, batch_size, file_paths, index, is_flattened = False):\n",
    "        ims = []\n",
    "        lbls = []\n",
    "        \n",
    "        while batch_size >= 1 and (len(file_paths) - index > 0):\n",
    "            im, lbl = self.load_image(file_paths[index], is_flattened)\n",
    "            ims.append(im)\n",
    "            lbls.append(lbl)\n",
    "            batch_size -= 1\n",
    "            index += 1\n",
    "        \n",
    "        return np.array(ims), np.array(lbls)\n",
    "\n",
    "    def train_data_loader(self, index):\n",
    "        return self.batch_data_loader(self.train_batch_size, self.train_paths, index)\n",
    "\n",
    "    def val_data_loader(self, index):\n",
    "        return self.batch_data_loader(self.val_batch_size, self.val_paths, index)\n",
    "\n",
    "    def test_data_loader(self, index):\n",
    "        return self.batch_data_loader(self.test_batch_size, self.test_paths, index)\n",
    "    \n",
    "    def get_train_data_size(self):\n",
    "        return len(self.train_paths)\n",
    "    \n",
    "    def get_val_data_size(self):\n",
    "        return len(self.val_paths)\n",
    "    \n",
    "    def get_test_data_size(self):\n",
    "        return len(self.test_paths)\n",
    "    \n",
    "    def all_train_data_loader(self, is_flattened = False):\n",
    "        return self.batch_data_loader(self.get_train_data_size(), self.train_paths, 0)\n",
    "    \n",
    "    def all_val_data_loader(self, is_flattened = False):\n",
    "        return self.batch_data_loader(self.get_val_data_size(), self.val_paths, 0)\n",
    "    \n",
    "    def all_test_data_loader(self, is_flattened = False):\n",
    "        return self.batch_data_loader(self.get_test_data_size(), self.test_paths, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "def create_dir(dir_name, relative_path):\n",
    "    \"\"\"\n",
    "    Create new directory if not exists\n",
    "    \n",
    "    Parameters:\n",
    "        dir_name (string)      - name of directory we want to create\n",
    "        relative_path (string) - absolute path of directory we want to create\n",
    "        \n",
    "    Returns:\n",
    "        path (string)          - full path of directory\n",
    "    \"\"\"\n",
    "    \n",
    "    path = relative_path + dir_name\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    return path\n",
    "\n",
    "# function to normalize data\n",
    "def normalize_data(data):\n",
    "    # scale features using statistics that are robust to outliers\n",
    "    #rs = sklearn.preprocessing.RobustScaler()\n",
    "    #rs.fit(data)\n",
    "    #data = rs.transform(data)\n",
    "    #data = (data-data.mean())/(data.std()) # standardisation\n",
    "    data = data / data.max() # convert from [0:255] to [0.:1.]\n",
    "    #data = ((data / 255.)-0.5)*2. # convert from [0:255] to [-1.:+1.]\n",
    "    return data\n",
    "\n",
    "# convert one-hot encodings into labels\n",
    "def one_hot_to_dense(labels_one_hot):\n",
    "    return np.argmax(labels_one_hot,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNN:\n",
    "    def __init__(self, train_images_dir, val_images_dir, test_images_dir, num_epochs, train_batch_size,\n",
    "                 val_batch_size, test_batch_size, height_of_image, width_of_image, num_channels, \n",
    "                 num_classes, learning_rate, base_dir, max_to_keep, model_name):\n",
    "\n",
    "        self.data_loader = DataLoader(train_images_dir, val_images_dir, test_images_dir, train_batch_size, \n",
    "                val_batch_size, test_batch_size, height_of_image, width_of_image, num_channels, num_classes)\n",
    "\n",
    "        self.num_epochs = num_epochs\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.height_of_image = height_of_image\n",
    "        self.width_of_image = width_of_image\n",
    "        self.num_channels = num_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.base_dir = base_dir\n",
    "        self.max_to_keep = max_to_keep\n",
    "        self.model_name = model_name\n",
    "\n",
    "        ####\n",
    "        self.keep_prob = 0.33 # keeping probability with dropout regularization \n",
    "        self.index_in_epoch = 0\n",
    "        self.current_epoch = 0\n",
    "        self.n_log_step = 0 # counting current number of mini batches trained on\n",
    "        \n",
    "        # permutation array\n",
    "        self.perm_array = np.array([])\n",
    "        ####\n",
    "        \n",
    "    # function to get the next mini batch\n",
    "    def next_mini_batch(self, mb_size):\n",
    "        start = self.index_in_epoch\n",
    "        self.index_in_epoch += mb_size\n",
    "        self.current_epoch += mb_size/len(self.x_train)  \n",
    "        \n",
    "        # adapt length of permutation array\n",
    "        if not len(self.perm_array) == len(self.x_train):\n",
    "            self.perm_array = np.arange(len(self.x_train))\n",
    "        \n",
    "        # shuffle once at the start of epoch\n",
    "        if start == 0:\n",
    "            np.random.shuffle(self.perm_array)\n",
    "\n",
    "        # at the end of the epoch\n",
    "        if self.index_in_epoch > self.x_train.shape[0]:\n",
    "            np.random.shuffle(self.perm_array) # shuffle data\n",
    "            start = 0 # start next epoch\n",
    "            self.index_in_epoch = mb_size # set index to mini batch size\n",
    "            \n",
    "            if self.train_on_augmented_data:\n",
    "                # use augmented data for the next epoch\n",
    "                self.x_train_aug = normalize_data(self.generate_images(self.x_train))\n",
    "                self.y_train_aug = self.y_train\n",
    "                \n",
    "        end = self.index_in_epoch\n",
    "        \n",
    "        if self.train_on_augmented_data:\n",
    "            # use augmented data\n",
    "            x_tr = self.x_train_aug[self.perm_array[start:end]]\n",
    "            y_tr = self.y_train_aug[self.perm_array[start:end]]\n",
    "        else:\n",
    "            # use original data\n",
    "            x_tr = self.x_train[self.perm_array[start:end]]\n",
    "            y_tr = self.y_train[self.perm_array[start:end]]\n",
    "        \n",
    "        return x_tr, y_tr\n",
    "    \n",
    "    # generate new images via rotations, translations, zoom using keras\n",
    "    def generate_images(self, imgs):\n",
    "    \n",
    "        print('generate new set of images')\n",
    "        \n",
    "        # rotations, translations, zoom\n",
    "        image_generator = keras.preprocessing.image.ImageDataGenerator(\n",
    "            rotation_range = 10, width_shift_range = 0.1 , height_shift_range = 0.1,\n",
    "            zoom_range = 0.1)\n",
    "\n",
    "        # get transformed images\n",
    "        imgs = image_generator.flow(imgs.copy(), np.zeros(len(imgs)),\n",
    "                                    batch_size=len(imgs), shuffle = False).next()    \n",
    "\n",
    "        return imgs[0]\n",
    "\n",
    "    # attach summaries to a tensor for TensorBoard visualization\n",
    "    def summary_variable(self, var, var_name):\n",
    "        with tf.name_scope(var_name):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "    \n",
    "    # function to create the network\n",
    "    def create_network(self):\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # variables for input and output \n",
    "        self.x_data_tf = tf.placeholder(dtype=tf.float32, shape=[None, self.height_of_image, self.width_of_image, self.num_channels], name='x_data_tf')\n",
    "        self.y_data_tf = tf.placeholder(dtype=tf.float32, shape=[None, self.num_classes], name='y_data_tf')\n",
    "\n",
    "        self.z_pred_tf = self.network(self.x_data_tf)\n",
    "\n",
    "        # cost function\n",
    "        self.cross_entropy_tf = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=self.y_data_tf, logits=self.z_pred_tf), name = 'cross_entropy_tf')\n",
    "     \n",
    "        # optimisation function\n",
    "        self.learn_rate_tf = tf.placeholder(dtype=tf.float32, name=\"learn_rate_tf\")\n",
    "        self.train_step_tf = tf.train.AdamOptimizer(self.learn_rate_tf).minimize(\n",
    "            self.cross_entropy_tf, name = 'train_step_tf')\n",
    "\n",
    "        # predicted probabilities in one-hot encoding\n",
    "        self.y_pred_proba_tf = tf.nn.softmax(self.z_pred_tf, name='y_pred_proba_tf') \n",
    "        \n",
    "        # tensor of correct predictions\n",
    "        self.y_pred_correct_tf = tf.equal(tf.argmax(self.y_pred_proba_tf, 1),\n",
    "                                          tf.argmax(self.y_data_tf, 1),\n",
    "                                          name = 'y_pred_correct_tf')  \n",
    "        \n",
    "        # accuracy \n",
    "        self.accuracy_tf = tf.reduce_mean(tf.cast(self.y_pred_correct_tf, dtype=tf.float32),\n",
    "                                         name = 'accuracy_tf')\n",
    "\n",
    "        # tensors to save intermediate accuracies and losses during training\n",
    "        self.train_loss_tf = tf.Variable(np.array([]), dtype=tf.float32, \n",
    "                                         name='train_loss_tf', validate_shape = False)\n",
    "        self.valid_loss_tf = tf.Variable(np.array([]), dtype=tf.float32,\n",
    "                                         name='valid_loss_tf', validate_shape = False)\n",
    "        self.train_acc_tf = tf.Variable(np.array([]), dtype=tf.float32, \n",
    "                                        name='train_acc_tf', validate_shape = False)\n",
    "        self.valid_acc_tf = tf.Variable(np.array([]), dtype=tf.float32, \n",
    "                                        name='valid_acc_tf', validate_shape = False)\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def attach_summary(self, sess):\n",
    "        \n",
    "        # create summary tensors for tensorboard\n",
    "        self.summary_variable(self.W_conv1_tf, 'W_conv1_tf')\n",
    "        self.summary_variable(self.b_conv1_tf, 'b_conv1_tf')\n",
    "        self.summary_variable(self.W_conv2_tf, 'W_conv2_tf')\n",
    "        self.summary_variable(self.b_conv2_tf, 'b_conv2_tf')\n",
    "        self.summary_variable(self.W_conv3_tf, 'W_conv3_tf')\n",
    "        self.summary_variable(self.b_conv3_tf, 'b_conv3_tf')\n",
    "        self.summary_variable(self.W_fc1_tf, 'W_fc1_tf')\n",
    "        self.summary_variable(self.b_fc1_tf, 'b_fc1_tf')\n",
    "        self.summary_variable(self.W_fc2_tf, 'W_fc2_tf')\n",
    "        self.summary_variable(self.b_fc2_tf, 'b_fc2_tf')\n",
    "        tf.summary.scalar('cross_entropy_tf', self.cross_entropy_tf)\n",
    "        tf.summary.scalar('accuracy_tf', self.accuracy_tf)\n",
    "\n",
    "        # merge all summaries for tensorboard\n",
    "        self.merged = tf.summary.merge_all()\n",
    "\n",
    "        # initialize summary writer \n",
    "        timestamp = datetime.datetime.now().strftime('%d-%m-%Y_%H-%M-%S')\n",
    "        filepath = os.path.join(os.getcwd(), self.base_dir, (self.model_name+'_'+timestamp))\n",
    "#         filepath = os.path.join(os.getcwd(), 'logs', (self.model_name+'_'+timestamp))\n",
    "        self.train_writer = tf.summary.FileWriter(os.path.join(filepath,'train'), sess.graph)\n",
    "        self.valid_writer = tf.summary.FileWriter(os.path.join(filepath,'valid'), sess.graph)\n",
    "\n",
    "    # helper function to train the model\n",
    "    def train_model_helper(self, sess, x_train, y_train, x_valid, y_valid, n_epoch = 1, \n",
    "                    train_on_augmented_data = False):        \n",
    "        # train on original or augmented data\n",
    "        self.train_on_augmented_data = train_on_augmented_data\n",
    "        \n",
    "        # use augmented data\n",
    "        if self.train_on_augmented_data:\n",
    "            print('generate new set of images')\n",
    "            self.x_train_aug = normalize_data(self.generate_images(self.x_train))\n",
    "            self.y_train_aug = self.y_train\n",
    "        \n",
    "        # parameters\n",
    "        mb_per_epoch = self.x_train.shape[0]/self.train_batch_size\n",
    "        train_loss, train_acc, valid_loss, valid_acc = [],[],[],[]\n",
    "        \n",
    "        # start timer\n",
    "        start = datetime.datetime.now();\n",
    "        print(datetime.datetime.now().strftime('%d-%m-%Y %H:%M:%S'),': start training')\n",
    "        print('learnrate = ',self.learning_rate,', n_epoch = ', n_epoch,\n",
    "              ', mb_size = ', self.train_batch_size)\n",
    "        # looping over mini batches\n",
    "        for i in range(int(n_epoch*mb_per_epoch)+1):            \n",
    "            # get new batch\n",
    "            x_batch, y_batch = self.next_mini_batch(self.train_batch_size)\n",
    "\n",
    "            # run the graph\n",
    "            sess.run(self.train_step_tf, feed_dict={self.x_data_tf: x_batch, \n",
    "                                                    self.y_data_tf: y_batch, \n",
    "                                                    self.keep_prob_tf: self.keep_prob, \n",
    "                                                    self.learn_rate_tf: self.learning_rate})\n",
    "            \n",
    "            feed_dict_valid = {self.x_data_tf: self.x_valid, \n",
    "                               self.y_data_tf: self.y_valid, \n",
    "                               self.keep_prob_tf: 1.0}\n",
    "#             feed_dict_train = {self.x_data_tf: self.x_train[self.perm_array[:len(self.x_valid)]], \n",
    "#                                 self.y_data_tf: self.y_train[self.perm_array[:len(self.y_valid)]], \n",
    "#                                 self.keep_prob_tf: 1.0}\n",
    "            feed_dict_train = {self.x_data_tf: x_batch, \n",
    "                                self.y_data_tf: y_batch, \n",
    "                                self.keep_prob_tf: 1.0}\n",
    "            \n",
    "            # store losses and accuracies\n",
    "            if i%self.validation_step == 0:\n",
    "                valid_loss.append(sess.run(self.cross_entropy_tf,\n",
    "                                           feed_dict = feed_dict_valid))\n",
    "                valid_acc.append(self.accuracy_tf.eval(session = sess, \n",
    "                                                       feed_dict = feed_dict_valid))\n",
    "                print('%.2f epoch, %.2f iteration: val loss = %.4f, val acc = %.4f'%(\n",
    "                    self.current_epoch, i, valid_loss[-1],valid_acc[-1]))\n",
    "                \n",
    "            # summary for tensorboard\n",
    "            if i%self.summary_step == 0:\n",
    "                self.n_log_step += 1 # for logging the results\n",
    "                train_summary = sess.run(self.merged, feed_dict={self.x_data_tf: x_batch, \n",
    "                                                                self.y_data_tf: y_batch, \n",
    "                                                                self.keep_prob_tf: 1.0})\n",
    "                valid_summary = sess.run(self.merged, feed_dict = feed_dict_valid)\n",
    "                self.train_writer.add_summary(train_summary, self.n_log_step)\n",
    "                self.valid_writer.add_summary(valid_summary, self.n_log_step)\n",
    "                \n",
    "            if i%self.display_step == 0:\n",
    "                train_loss.append(sess.run(self.cross_entropy_tf,\n",
    "                                           feed_dict = feed_dict_train))\n",
    "                train_acc.append(self.accuracy_tf.eval(session = sess, \n",
    "                                                       feed_dict = feed_dict_train))\n",
    "                print('%.2f epoch, %.2f iteration: train loss = %.4f, train acc = %.4f'%(\n",
    "                    self.current_epoch, i,  train_loss[-1],train_acc[-1]))\n",
    "                \n",
    "            # save tensors and summaries of model\n",
    "            if i%self.checkpoint_step == 0:\n",
    "                self.save_model(sess)\n",
    "                \n",
    "        # concatenate losses and accuracies and assign to tensor variables\n",
    "        tl_c = np.concatenate([self.train_loss_tf.eval(session=sess), train_loss], axis = 0)\n",
    "        vl_c = np.concatenate([self.valid_loss_tf.eval(session=sess), valid_loss], axis = 0)\n",
    "        ta_c = np.concatenate([self.train_acc_tf.eval(session=sess), train_acc], axis = 0)\n",
    "        va_c = np.concatenate([self.valid_acc_tf.eval(session=sess), valid_acc], axis = 0)\n",
    "   \n",
    "        sess.run(tf.assign(self.train_loss_tf, tl_c, validate_shape = False))\n",
    "        sess.run(tf.assign(self.valid_loss_tf, vl_c , validate_shape = False))\n",
    "        sess.run(tf.assign(self.train_acc_tf, ta_c , validate_shape = False))\n",
    "        sess.run(tf.assign(self.valid_acc_tf, va_c , validate_shape = False))\n",
    "        \n",
    "        print('running time for training: ', datetime.datetime.now() - start)\n",
    "        return None\n",
    "    \n",
    "    def train_model(self, display_step, validation_step, checkpoint_step, summary_step):\n",
    "        self.display_step = display_step\n",
    "        self.validation_step = validation_step\n",
    "        self.checkpoint_step = checkpoint_step\n",
    "        self.summary_step = summary_step\n",
    "        \n",
    "        # training and validation data\n",
    "        self.x_train, self.y_train = self.data_loader.all_train_data_loader()\n",
    "        self.x_valid, self.y_valid = self.data_loader.all_val_data_loader()\n",
    "\n",
    "        self.x_train = self.x_train.reshape(-1, self.height_of_image, self.width_of_image, self.num_channels)\n",
    "        self.x_valid = self.x_valid.reshape(-1, self.height_of_image, self.width_of_image, self.num_channels)\n",
    "\n",
    "        self.saver_tf = tf.train.Saver()\n",
    "        # start timer\n",
    "        start = datetime.datetime.now();\n",
    "\n",
    "        # start tensorflow session\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            # attach summaries\n",
    "            self.attach_summary(sess)\n",
    "\n",
    "            # variable initialization of the default graph\n",
    "            sess.run(tf.global_variables_initializer()) \n",
    "\n",
    "            # training on original data\n",
    "            self.train_model_helper(sess, x_train, y_train, x_valid, y_valid, n_epoch = self.num_epochs)\n",
    "\n",
    "            # training on augmented data\n",
    "#             self.train_model_helper(sess, x_train, y_train, x_valid, y_valid, n_epoch = 14.0,\n",
    "#                                 train_on_augmented_data = True)\n",
    "\n",
    "            # save tensors and summaries of model\n",
    "            self.save_model(sess)\n",
    "\n",
    "        print('total running time for training: ', datetime.datetime.now() - start)\n",
    "\n",
    "    # save tensors/summaries\n",
    "    def save_model(self, sess):\n",
    "        # tf saver\n",
    "        filepath = os.path.join(os.getcwd(), self.model_name)\n",
    "        self.saver_tf.save(sess, filepath)\n",
    "        # tb summary\n",
    "        self.train_writer.close()\n",
    "        self.valid_writer.close()\n",
    "        \n",
    "        return None\n",
    "  \n",
    "    # forward prediction of current graph\n",
    "    def forward(self, sess, x_data):\n",
    "        y_pred_proba = self.y_pred_proba_tf.eval(session = sess, \n",
    "                                                 feed_dict = {self.x_data_tf: x_data,\n",
    "                                                              self.keep_prob_tf: 1.0})\n",
    "        return y_pred_proba\n",
    "    \n",
    "    # load session from file, restore graph, and load tensors\n",
    "    def load_session_from_file(self, filename):\n",
    "        tf.reset_default_graph()\n",
    "        filepath = os.path.join(os.getcwd(), filename + '.meta')\n",
    "        saver = tf.train.import_meta_graph(filepath)\n",
    "        print(filepath)\n",
    "        sess = tf.Session()\n",
    "        saver.restore(sess, self.model_name)\n",
    "        graph = tf.get_default_graph()\n",
    "        self.load_tensors(graph)\n",
    "        return sess\n",
    "    \n",
    "    def test_model(self):\n",
    "        x_test, y_test = self.data_loader.all_test_data_loader()\n",
    "        x_test = x_test.reshape(-1, self.height_of_image, self.width_of_image, self.num_channels)\n",
    "        \n",
    "        sess = self.load_session_from_file(self.model_name) # receive session \n",
    "        y_test_pred = {}\n",
    "        y_test_pred_labels = {}\n",
    "        y_test_pred[self.model_name] = self.forward(sess, x_test)\n",
    "        sess.close()\n",
    "        y_test_pred_labels[self.model_name] = one_hot_to_dense(y_test_pred[self.model_name])\n",
    "        y_test = one_hot_to_dense(y_test)\n",
    "        \n",
    "        print('Test Accuracy: ', self.metrics(y_test, y_test_pred_labels[self.model_name]))\n",
    "        return self.metrics(y_test, y_test_pred_labels[self.model_name])\n",
    "        \n",
    "    \n",
    "    def initialize_network(self):\n",
    "        self.load_session_from_file(self.model_name)\n",
    "        return None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def network(self, X):\n",
    "        raise NotImplementedError('subclasses must override network()!')\n",
    "\n",
    "    @abstractmethod\n",
    "    def metrics(self, Y, y_pred):\n",
    "        raise NotImplementedError('subclasses must override metrics()!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .BaseNN import *\n",
    "\n",
    "class DNN(BaseNN):\n",
    "\n",
    "    def __init__(self, train_images_dir, val_images_dir, test_images_dir, num_epochs, train_batch_size,\n",
    "                 val_batch_size, test_batch_size, height_of_image, width_of_image, num_channels, \n",
    "                 num_classes, learning_rate, base_dir, max_to_keep, model_name):\n",
    "\n",
    "        super().__init__(train_images_dir, val_images_dir, test_images_dir, num_epochs, train_batch_size,\n",
    "                 val_batch_size, test_batch_size, height_of_image, width_of_image, num_channels, \n",
    "                 num_classes, learning_rate, base_dir, max_to_keep, model_name)\n",
    "\n",
    "        # tunable hyperparameters for nn architecture\n",
    "        self.s_f_conv1 = 3; # filter size of first convolution layer (default = 3)\n",
    "        self.n_f_conv1 = 36; # number of features of first convolution layer (default = 36)\n",
    "        self.s_f_conv2 = 3; # filter size of second convolution layer (default = 3)\n",
    "        self.n_f_conv2 = 36; # number of features of second convolution layer (default = 36)\n",
    "        self.s_f_conv3 = 3; # filter size of third convolution layer (default = 3)\n",
    "        self.n_f_conv3 = 36; # number of features of third convolution layer (default = 36)\n",
    "        self.n_n_fc1 = 576; # number of neurons of first fully connected layer (default = 576)\n",
    "\n",
    "    # weight initialization\n",
    "    def weight_variable(self, shape, name = None):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial, name = name)\n",
    "\n",
    "    # bias initialization\n",
    "    def bias_variable(self, shape, name = None):\n",
    "        initial = tf.constant(0.1, shape=shape) #  positive bias\n",
    "        return tf.Variable(initial, name = name)\n",
    "\n",
    "    # 2D convolution\n",
    "    def conv2d(self, x, W, name = None):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name = name)\n",
    "\n",
    "    # max pooling\n",
    "    def max_pool_2x2(self, x, name = None):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name = name)\n",
    "    \n",
    "    # function to load tensors from a saved graph\n",
    "    def load_tensors(self, graph):\n",
    "        \n",
    "        # input tensors\n",
    "        self.x_data_tf = graph.get_tensor_by_name(\"x_data_tf:0\")\n",
    "        self.y_data_tf = graph.get_tensor_by_name(\"y_data_tf:0\")\n",
    "        \n",
    "        # weights and bias tensors\n",
    "        self.W_conv1_tf = graph.get_tensor_by_name(\"W_conv1_tf:0\")\n",
    "        self.W_conv2_tf = graph.get_tensor_by_name(\"W_conv2_tf:0\")\n",
    "        self.W_conv3_tf = graph.get_tensor_by_name(\"W_conv3_tf:0\")\n",
    "        self.W_fc1_tf = graph.get_tensor_by_name(\"W_fc1_tf:0\")\n",
    "        self.W_fc2_tf = graph.get_tensor_by_name(\"W_fc2_tf:0\")\n",
    "        self.b_conv1_tf = graph.get_tensor_by_name(\"b_conv1_tf:0\")\n",
    "        self.b_conv2_tf = graph.get_tensor_by_name(\"b_conv2_tf:0\")\n",
    "        self.b_conv3_tf = graph.get_tensor_by_name(\"b_conv3_tf:0\")\n",
    "        self.b_fc1_tf = graph.get_tensor_by_name(\"b_fc1_tf:0\")\n",
    "        self.b_fc2_tf = graph.get_tensor_by_name(\"b_fc2_tf:0\")\n",
    "        \n",
    "        # activation tensors\n",
    "        self.h_conv1_tf = graph.get_tensor_by_name('h_conv1_tf:0')  \n",
    "        self.h_pool1_tf = graph.get_tensor_by_name('h_pool1_tf:0')\n",
    "        self.h_conv2_tf = graph.get_tensor_by_name('h_conv2_tf:0')\n",
    "        self.h_pool2_tf = graph.get_tensor_by_name('h_pool2_tf:0')\n",
    "        self.h_conv3_tf = graph.get_tensor_by_name('h_conv3_tf:0')\n",
    "        self.h_pool3_tf = graph.get_tensor_by_name('h_pool3_tf:0')\n",
    "        self.h_fc1_tf = graph.get_tensor_by_name('h_fc1_tf:0')\n",
    "        self.z_pred_tf = graph.get_tensor_by_name('z_pred_tf:0')\n",
    "        \n",
    "        # training and prediction tensors\n",
    "        self.learn_rate_tf = graph.get_tensor_by_name(\"learn_rate_tf:0\")\n",
    "        self.keep_prob_tf = graph.get_tensor_by_name(\"keep_prob_tf:0\")\n",
    "        self.cross_entropy_tf = graph.get_tensor_by_name('cross_entropy_tf:0')\n",
    "        self.train_step_tf = graph.get_operation_by_name('train_step_tf')\n",
    "        self.z_pred_tf = graph.get_tensor_by_name('z_pred_tf:0')\n",
    "        self.y_pred_proba_tf = graph.get_tensor_by_name(\"y_pred_proba_tf:0\")\n",
    "        self.y_pred_correct_tf = graph.get_tensor_by_name('y_pred_correct_tf:0')\n",
    "        self.accuracy_tf = graph.get_tensor_by_name('accuracy_tf:0')\n",
    "        \n",
    "        # tensor of stored losses and accuricies during training\n",
    "        self.train_loss_tf = graph.get_tensor_by_name(\"train_loss_tf:0\")\n",
    "        self.train_acc_tf = graph.get_tensor_by_name(\"train_acc_tf:0\")\n",
    "        self.valid_loss_tf = graph.get_tensor_by_name(\"valid_loss_tf:0\")\n",
    "        self.valid_acc_tf = graph.get_tensor_by_name(\"valid_acc_tf:0\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    def network(self, X):\n",
    "#         tf.reset_default_graph()\n",
    "\n",
    "        # 1.layer: convolution + max pooling\n",
    "        self.W_conv1_tf = self.weight_variable([self.s_f_conv1, self.s_f_conv1, 1, self.n_f_conv1], name = 'W_conv1_tf') # (5,5,1,32)\n",
    "        self.b_conv1_tf = self.bias_variable([self.n_f_conv1], name = 'b_conv1_tf') # (32)\n",
    "        self.h_conv1_tf = tf.nn.relu(self.conv2d(X, self.W_conv1_tf) + self.b_conv1_tf, name = 'h_conv1_tf') # (.,28,28,32)\n",
    "        self.h_pool1_tf = self.max_pool_2x2(self.h_conv1_tf, name = 'h_pool1_tf') # (.,14,14,32)\n",
    "\n",
    "        # 2.layer: convolution + max pooling\n",
    "        self.W_conv2_tf = self.weight_variable([self.s_f_conv2, self.s_f_conv2, self.n_f_conv1, self.n_f_conv2], name = 'W_conv2_tf')\n",
    "        self.b_conv2_tf = self.bias_variable([self.n_f_conv2], name = 'b_conv2_tf')\n",
    "        self.h_conv2_tf = tf.nn.relu(self.conv2d(self.h_pool1_tf, self.W_conv2_tf) + self.b_conv2_tf, name ='h_conv2_tf') #(.,14,14,32)\n",
    "        self.h_pool2_tf = self.max_pool_2x2(self.h_conv2_tf, name = 'h_pool2_tf') #(.,7,7,32)\n",
    "\n",
    "        # 3.layer: convolution + max pooling\n",
    "        self.W_conv3_tf = self.weight_variable([self.s_f_conv3, self.s_f_conv3, self.n_f_conv2, self.n_f_conv3], name = 'W_conv3_tf')\n",
    "        self.b_conv3_tf = self.bias_variable([self.n_f_conv3], name = 'b_conv3_tf')\n",
    "        self.h_conv3_tf = tf.nn.relu(self.conv2d(self.h_pool2_tf, self.W_conv3_tf) + self.b_conv3_tf, name = 'h_conv3_tf') #(.,7,7,32)\n",
    "        self.h_pool3_tf = self.max_pool_2x2(self.h_conv3_tf, name = 'h_pool3_tf') # (.,4,4,32)\n",
    "\n",
    "        # 4.layer: fully connected\n",
    "        self.W_fc1_tf = self.weight_variable([4*4*self.n_f_conv3,self.n_n_fc1], name = 'W_fc1_tf') # (4*4*32, 1024)\n",
    "        self.b_fc1_tf = self.bias_variable([self.n_n_fc1], name = 'b_fc1_tf') # (1024)\n",
    "        self.h_pool3_flat_tf = tf.reshape(self.h_pool3_tf, [-1,4*4*self.n_f_conv3], name = 'h_pool3_flat_tf') # (.,1024)\n",
    "        self.h_fc1_tf = tf.nn.relu(tf.matmul(self.h_pool3_flat_tf, self.W_fc1_tf) + self.b_fc1_tf, name = 'h_fc1_tf') # (.,1024)\n",
    "      \n",
    "        # add dropout\n",
    "        self.keep_prob_tf = tf.placeholder(dtype=tf.float32, name = 'keep_prob_tf')\n",
    "        self.h_fc1_drop_tf = tf.nn.dropout(self.h_fc1_tf, self.keep_prob_tf, name = 'h_fc1_drop_tf')\n",
    "\n",
    "        # 5.layer: fully connected\n",
    "        self.W_fc2_tf = self.weight_variable([self.n_n_fc1, 10], name = 'W_fc2_tf')\n",
    "        self.b_fc2_tf = self.bias_variable([10], name = 'b_fc2_tf')\n",
    "        \n",
    "        self.z_pred_tf = tf.add(tf.matmul(self.h_fc1_drop_tf, self.W_fc2_tf), self.b_fc2_tf, name = 'z_pred_tf')# => (.,10)\n",
    "\n",
    "        return self.z_pred_tf\n",
    "\n",
    "    def metrics(self, Y, Y_pred):\n",
    "        Y = Y.reshape(-1,)\n",
    "        Y_pred = Y_pred.reshape(-1,)\n",
    "        return np.mean(Y == Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_graph = DNN(\n",
    "    train_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/train/image/',\n",
    "    val_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/validation/image/',\n",
    "    test_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/test/image/',\n",
    "    num_epochs=1,\n",
    "    train_batch_size=100,\n",
    "    val_batch_size=100,\n",
    "    test_batch_size=100,\n",
    "    height_of_image=28,\n",
    "    width_of_image=28,\n",
    "    num_channels=1,\n",
    "    num_classes=10,\n",
    "    learning_rate = 0.001,\n",
    "    base_dir='results',\n",
    "    max_to_keep=5,\n",
    "    model_name='nn_1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_graph.create_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\_Files\\MyProjects\\ASDS_3\\ASDS_DL\\Homeworks\\3_mnist\\ddxk\\nn_1.meta\n"
     ]
    }
   ],
   "source": [
    "nn_graph.initialize_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30-10-2019 01:15:59 : start training\n",
      "learnrate =  0.001 , n_epoch =  1 , mb_size =  100\n",
      "0.00 epoch, 0.00 iteration: val loss = 81.0331, val acc = 0.1615\n",
      "0.00 epoch, 0.00 iteration: train loss = 64.4220, train acc = 0.2200\n",
      "0.21 epoch, 100.00 iteration: val loss = 0.7466, val acc = 0.7975\n",
      "0.21 epoch, 100.00 iteration: train loss = 0.5520, train acc = 0.8200\n",
      "0.42 epoch, 200.00 iteration: val loss = 0.4537, val acc = 0.8655\n",
      "0.42 epoch, 200.00 iteration: train loss = 0.4581, train acc = 0.8100\n",
      "0.63 epoch, 300.00 iteration: val loss = 0.3201, val acc = 0.8998\n",
      "0.63 epoch, 300.00 iteration: train loss = 0.3719, train acc = 0.8800\n",
      "0.84 epoch, 400.00 iteration: val loss = 0.2629, val acc = 0.9197\n",
      "0.84 epoch, 400.00 iteration: train loss = 0.2183, train acc = 0.9100\n",
      "running time for training:  0:01:57.217938\n",
      "total running time for training:  0:01:58.971024\n"
     ]
    }
   ],
   "source": [
    "# display_step, validation_step, checkpoint_step, summary_step\n",
    "nn_graph.train_model(100, 100, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\_Files\\MyProjects\\ASDS_3\\ASDS_DL\\Homeworks\\3_mnist\\ddxk\\nn_1.meta\n",
      "Test Accuracy:  0.929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.929"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_graph.test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/train/image/'\n",
    "val_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/validation/image/'\n",
    "test_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/test/image/'\n",
    "num_epochs=2\n",
    "train_batch_size=100\n",
    "val_batch_size=100\n",
    "test_batch_size=100\n",
    "height_of_image=28\n",
    "width_of_image=28\n",
    "num_channels=1\n",
    "num_classes=10\n",
    "learning_rate = 0.001\n",
    "base_dir='./results'\n",
    "max_to_keep=5\n",
    "model_name='nn_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader = DataLoader(train_images_dir, val_images_dir, test_images_dir, train_batch_size, \n",
    "#                 val_batch_size, test_batch_size, height_of_image, width_of_image, num_channels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train = data_loader.all_train_data_loader()\n",
    "# x_valid, y_valid = data_loader.all_val_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train.reshape(-1,28,28,1)\n",
    "# x_valid = x_valid.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30-10-2019 00:36:25 : start training\n",
      "learnrate =  0.001 , n_epoch =  1.0 , mb_size =  100\n",
      "0.00 epoch, 0.00 iteration: val loss = 131.0541, val acc = 0.1503\n",
      "0.00 epoch, 0.00 iteration: train loss = 114.6608, train acc = 0.1700\n",
      "0.21 epoch, 100.00 iteration: val loss = 0.7538, val acc = 0.8490\n",
      "0.21 epoch, 100.00 iteration: train loss = 1.0192, train acc = 0.8200\n",
      "0.42 epoch, 200.00 iteration: val loss = 0.3901, val acc = 0.9041\n",
      "0.42 epoch, 200.00 iteration: train loss = 0.3958, train acc = 0.8900\n",
      "0.63 epoch, 300.00 iteration: val loss = 0.2748, val acc = 0.9227\n",
      "0.63 epoch, 300.00 iteration: train loss = 0.2503, train acc = 0.9300\n",
      "0.84 epoch, 400.00 iteration: val loss = 0.2269, val acc = 0.9370\n",
      "0.84 epoch, 400.00 iteration: train loss = 0.4802, train acc = 0.8800\n",
      "running time for training:  0:01:16.071384\n",
      "total running time for training:  0:01:17.736974\n"
     ]
    }
   ],
   "source": [
    "# start timer\n",
    "start = datetime.datetime.now();\n",
    "\n",
    "nn_graph = DNN(\n",
    "    train_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/train/image/',\n",
    "    val_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/validation/image/',\n",
    "    test_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/test/image/',\n",
    "    num_epochs=2,\n",
    "    train_batch_size=100,\n",
    "    val_batch_size=100,\n",
    "    test_batch_size=100,\n",
    "    height_of_image=28,\n",
    "    width_of_image=28,\n",
    "    num_channels=1,\n",
    "    num_classes=10,\n",
    "    learning_rate = 0.001,\n",
    "    base_dir='./results',\n",
    "    max_to_keep=5,\n",
    "    model_name='nn_1'\n",
    ")\n",
    "\n",
    "nn_graph.create_network() # create graph\n",
    "nn_graph.attach_saver() # attach saver tensors\n",
    "\n",
    "# start tensorflow session\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # attach summaries\n",
    "    nn_graph.attach_summary(sess) \n",
    "\n",
    "    # variable initialization of the default graph\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "\n",
    "    # training on original data\n",
    "    nn_graph.train_model_helper(sess, x_train, y_train, x_valid, y_valid, n_epoch = 1.0)\n",
    "\n",
    "    # training on augmented data\n",
    "#     nn_graph.train_graph_helper(sess, x_train, y_train, x_valid, y_valid, n_epoch = 14.0,\n",
    "#                         train_on_augmented_data = True)\n",
    "\n",
    "    # save tensors and summaries of model\n",
    "    nn_graph.save_model(sess)\n",
    "\n",
    "print('total running time for training: ', datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = data_loader.all_test_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\_Files\\MyProjects\\ASDS_3\\ASDS_DL\\Homeworks\\3_mnist\\ddxk\\nn_1.meta\n"
     ]
    }
   ],
   "source": [
    "mn = 'nn_1' # choose saved model\n",
    "\n",
    "nn_graph = DNN(\n",
    "    train_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/train/image/',\n",
    "    val_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/validation/image/',\n",
    "    test_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/test/image/',\n",
    "    num_epochs=2,\n",
    "    train_batch_size=100,\n",
    "    val_batch_size=100,\n",
    "    test_batch_size=100,\n",
    "    height_of_image=28,\n",
    "    width_of_image=28,\n",
    "    num_channels=1,\n",
    "    num_classes=10,\n",
    "    learning_rate = 0.001,\n",
    "    base_dir='results',\n",
    "    max_to_keep=5,\n",
    "    model_name='nn_1'\n",
    ")\n",
    "\n",
    "sess = nn_graph.load_session_from_file(mn) # receive session \n",
    "y_test_pred = {}\n",
    "y_test_pred_labels = {}\n",
    "y_test_pred[mn] = nn_graph.forward(sess, x_test[valid_index])\n",
    "sess.close()\n",
    "y_test_pred_labels[mn] = one_hot_to_dense(y_test_pred[mn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nn_1': array([[1.0644753e-04, 6.5491321e-08, 9.9973804e-01, ..., 1.3535220e-05,\n",
       "         1.3180485e-07, 3.4908123e-08],\n",
       "        [6.2340061e-04, 9.4191164e-06, 2.5706730e-04, ..., 1.1052801e-05,\n",
       "         9.7747600e-01, 2.0600488e-02],\n",
       "        [8.6418251e-05, 8.3974435e-04, 1.2003451e-03, ..., 9.9064189e-01,\n",
       "         1.4619529e-04, 4.0887035e-03],\n",
       "        ...,\n",
       "        [1.9270385e-06, 5.8349764e-07, 2.5850793e-04, ..., 8.0621731e-01,\n",
       "         3.6295105e-04, 3.5466946e-05],\n",
       "        [9.7872046e-08, 2.9188033e-05, 3.9759030e-05, ..., 9.9931908e-01,\n",
       "         3.7208779e-06, 1.2340343e-04],\n",
       "        [9.9996805e-01, 6.9281998e-09, 1.1446301e-06, ..., 2.4361521e-05,\n",
       "         7.6764906e-08, 3.1657811e-07]], dtype=float32)}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn_1: y_test_pred_labels[mn].shape =  (10000,)\n",
      "{0: 1019, 1: 1125, 2: 1027, 3: 998, 4: 960, 5: 919, 6: 958, 7: 997, 8: 984, 9: 1013}\n"
     ]
    }
   ],
   "source": [
    "# choose the test predictions and submit the results\n",
    "\n",
    "mn = 'nn_1'\n",
    "y_test_pred_labels[mn] = one_hot_to_dense(y_test_pred[mn])\n",
    "\n",
    "print(mn+': y_test_pred_labels[mn].shape = ', y_test_pred_labels[mn].shape)\n",
    "unique, counts = np.unique(y_test_pred_labels[mn], return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "# save predictions\n",
    "# np.savetxt('submission.csv', \n",
    "#            np.c_[range(1,len(x_test)+1), y_test_pred_labels[mn]], \n",
    "#            delimiter=',', \n",
    "#            header = 'ImageId,Label', \n",
    "#            comments = '', \n",
    "#            fmt='%d')\n",
    "\n",
    "# print('submission.csv completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = one_hot_to_dense(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9391"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_from_dense_labels(y_test, y_test_pred_labels[mn])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

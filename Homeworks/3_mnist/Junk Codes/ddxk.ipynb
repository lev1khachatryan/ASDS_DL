{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras.preprocessing.image\n",
    "# import sklearn.preprocessing\n",
    "import os;\n",
    "import datetime  \n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "# from data_loader import *\n",
    "from abc import abstractmethod\n",
    "from utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "def create_dir(dir_name, relative_path):\n",
    "    \"\"\"\n",
    "    Create new directory if not exists\n",
    "    --------------\n",
    "    Parameters:\n",
    "        dir_name (string)      - name of directory we want to create\n",
    "        relative_path (string) - absolute path of directory we want to create\n",
    "    Returns:\n",
    "        path (string)          - full path of directory\n",
    "    --------------\n",
    "    \"\"\"\n",
    "    \n",
    "    path = relative_path + dir_name\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    return path\n",
    "\n",
    "def normalize_data(data):\n",
    "    \"\"\"\n",
    "    function to normalize data\n",
    "    --------------\n",
    "    Parameters:\n",
    "        data (dataframe or matrix_like) - data we want to normalize, for example images\n",
    "    Returns:\n",
    "        dataframe or matrix_like (normalized data)\n",
    "    --------------\n",
    "    \"\"\"\n",
    "    # scale features using statistics that are robust to outliers\n",
    "    #rs = sklearn.preprocessing.RobustScaler()\n",
    "    #rs.fit(data)\n",
    "    #data = rs.transform(data)\n",
    "    #data = (data-data.mean())/(data.std()) # standardisation\n",
    "    data = data / data.max() # convert from [0:255] to [0.:1.]\n",
    "    #data = ((data / 255.)-0.5)*2. # convert from [0:255] to [-1.:+1.]\n",
    "    return data\n",
    "\n",
    "def one_hot_to_dense(labels_one_hot):\n",
    "    \"\"\"\n",
    "    convert one-hot encodings into labels\n",
    "    --------------\n",
    "    Parameters:\n",
    "        labels_one_hot (matrix_like) - all one hot vectors we want to encode\n",
    "    Returns:\n",
    "        int (encoded value)\n",
    "    --------------\n",
    "    \"\"\"\n",
    "    return np.argmax(labels_one_hot,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, train_images_dir, val_images_dir, test_images_dir, train_batch_size, val_batch_size, \n",
    "            test_batch_size, height_of_image, width_of_image, num_channels, num_classes):\n",
    "\n",
    "        self.train_paths = glob.glob(os.path.join(train_images_dir, \"**/*.png\"), recursive=True)\n",
    "        self.val_paths = glob.glob(os.path.join(val_images_dir, \"**/*.png\"), recursive=True)\n",
    "        self.test_paths = glob.glob(os.path.join(test_images_dir, \"**/*.png\"), recursive=True)\n",
    "\n",
    "#         random.shuffle(self.train_paths)\n",
    "#         random.shuffle(self.val_paths)\n",
    "#         random.shuffle(self.test_paths)\n",
    "\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "\n",
    "        self.height_of_image = height_of_image\n",
    "        self.width_of_image = width_of_image\n",
    "        self.num_channels = num_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def load_image(self, path, is_flattened = False):\n",
    "        im = np.asarray(Image.open(path))\n",
    "        lbl = np.eye(self.num_classes)[int(path.rsplit('\\\\', 2)[-2])]\n",
    "\n",
    "        if is_flattened:\n",
    "            im = im.reshape(self.height_of_image * self.width_of_image)\n",
    "\n",
    "        return im, lbl\n",
    "\n",
    "    def batch_data_loader(self, batch_size, file_paths, index, is_flattened = False, randomization = False):\n",
    "        ims = []\n",
    "        lbls = []\n",
    "        \n",
    "        if index == 0 or randomization:\n",
    "            random.shuffle(file_paths)\n",
    "        \n",
    "        while batch_size >= 1 and (len(file_paths) - index > 0):\n",
    "            im, lbl = self.load_image(file_paths[index], is_flattened)\n",
    "            ims.append(im)\n",
    "            lbls.append(lbl)\n",
    "            batch_size -= 1\n",
    "            index += 1\n",
    "        imgs = np.array(ims)\n",
    "        imgs = imgs.reshape(-1, self.height_of_image, self.width_of_image, self.num_channels)\n",
    "\n",
    "        return imgs, np.array(lbls)\n",
    "\n",
    "    def train_data_loader(self, index, randomization = False):\n",
    "        return self.batch_data_loader(self.train_batch_size, self.train_paths, index, randomization = randomization)\n",
    "\n",
    "    def val_data_loader(self, index, randomization = False):\n",
    "        return self.batch_data_loader(self.val_batch_size, self.val_paths, index, randomization = randomization)\n",
    "\n",
    "    def test_data_loader(self, index, randomization = False):\n",
    "        return self.batch_data_loader(self.test_batch_size, self.test_paths, index, randomization = randomization)\n",
    "    \n",
    "    def get_train_data_size(self):\n",
    "        return len(self.train_paths)\n",
    "    \n",
    "    def get_val_data_size(self):\n",
    "        return len(self.val_paths)\n",
    "    \n",
    "    def get_test_data_size(self):\n",
    "        return len(self.test_paths)\n",
    "    \n",
    "    def all_train_data_loader(self, is_flattened = False):\n",
    "        return self.batch_data_loader(self.get_train_data_size(), self.train_paths, 0)\n",
    "    \n",
    "    def all_val_data_loader(self, is_flattened = False):\n",
    "        return self.batch_data_loader(self.get_val_data_size(), self.val_paths, 0)\n",
    "    \n",
    "    def all_test_data_loader(self, is_flattened = False):\n",
    "        return self.batch_data_loader(self.get_test_data_size(), self.test_paths, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .BaseNN import *\n",
    "\n",
    "class DNN(BaseNN):\n",
    "\n",
    "    def __init__(self, train_images_dir, val_images_dir, test_images_dir, num_epochs, train_batch_size,\n",
    "                 val_batch_size, test_batch_size, height_of_image, width_of_image, num_channels, \n",
    "                 num_classes, learning_rate, base_dir, max_to_keep, model_name, keep_prob):\n",
    "\n",
    "        super().__init__(train_images_dir, val_images_dir, test_images_dir, num_epochs, train_batch_size,\n",
    "                 val_batch_size, test_batch_size, height_of_image, width_of_image, num_channels, \n",
    "                 num_classes, learning_rate, base_dir, max_to_keep, model_name, keep_prob)\n",
    "\n",
    "        self.s_f_conv1 = 3; # filter size of first convolution layer (default = 3)\n",
    "        self.n_f_conv1 = 36; # number of features of first convolution layer (default = 36)\n",
    "        self.s_f_conv2 = 3; # filter size of second convolution layer (default = 3)\n",
    "        self.n_f_conv2 = 36; # number of features of second convolution layer (default = 36)\n",
    "        self.s_f_conv3 = 3; # filter size of third convolution layer (default = 3)\n",
    "        self.n_f_conv3 = 36; # number of features of third convolution layer (default = 36)\n",
    "        self.n_n_fc1 = 576; # number of neurons of first fully connected layer (default = 576)\n",
    "\n",
    "    def weight_variable(self, shape, name = None):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            shape   (tuple)     - shape of weight variable\n",
    "            name    (string)    - name of weight variable\n",
    "        Returns:\n",
    "            tf.Variable         - initialized weight variable\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial, name = name)\n",
    "\n",
    "    def bias_variable(self, shape, name = None):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            shape   (tuple)     - shape of bias variable\n",
    "            name    (string)    - name of bias variable\n",
    "        Returns:\n",
    "            tf.Variable         - initialized bias variable\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        initial = tf.constant(0.1, shape=shape) #  positive bias\n",
    "        return tf.Variable(initial, name = name)\n",
    "\n",
    "    def conv2d(self, x, W, name = None):\n",
    "        \"\"\"\n",
    "        2D convolution\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            x       (matrix-like)   - input data\n",
    "            W       (tf.Variable)   - Weight matrix\n",
    "            name    (string)        - name of the graph node\n",
    "        Returns:\n",
    "            tf.Conv                 - Convolution response\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name = name)\n",
    "\n",
    "    def max_pool_2x2(self, x, name = None):\n",
    "        \"\"\"\n",
    "        2 x 2 max pooling\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            x       (matrix-like)   - matrix we want to apply max pooling\n",
    "            name    (string)        - name of the graph node\n",
    "        Returns:\n",
    "            tf.max_pool             - max pooling response\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name = name)\n",
    "\n",
    "    def summary_variable(self, var, var_name):\n",
    "        \"\"\"\n",
    "        Attach summaries to a tensor for TensorBoard visualization\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            var         - variable we want to attach\n",
    "            var_name    - name of the variable\n",
    "        Returns:\n",
    "            None\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        with tf.name_scope(var_name):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "        return None\n",
    "    \n",
    "    def load_tensors(self, graph):\n",
    "        \"\"\"\n",
    "        load tensors from a saved graph\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            graph       (tf.graph_like) - graph we obtained from saved file\n",
    "        Returns:\n",
    "            None\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "\n",
    "        # input tensors\n",
    "        self.x_data_tf = graph.get_tensor_by_name(\"x_data_tf:0\")\n",
    "        self.y_data_tf = graph.get_tensor_by_name(\"y_data_tf:0\")\n",
    "        \n",
    "        # weights and bias tensors\n",
    "        self.W_conv1_tf = graph.get_tensor_by_name(\"W_conv1_tf:0\")\n",
    "        self.W_conv2_tf = graph.get_tensor_by_name(\"W_conv2_tf:0\")\n",
    "        self.W_conv3_tf = graph.get_tensor_by_name(\"W_conv3_tf:0\")\n",
    "        self.W_fc1_tf = graph.get_tensor_by_name(\"W_fc1_tf:0\")\n",
    "        self.W_fc2_tf = graph.get_tensor_by_name(\"W_fc2_tf:0\")\n",
    "        self.b_conv1_tf = graph.get_tensor_by_name(\"b_conv1_tf:0\")\n",
    "        self.b_conv2_tf = graph.get_tensor_by_name(\"b_conv2_tf:0\")\n",
    "        self.b_conv3_tf = graph.get_tensor_by_name(\"b_conv3_tf:0\")\n",
    "        self.b_fc1_tf = graph.get_tensor_by_name(\"b_fc1_tf:0\")\n",
    "        self.b_fc2_tf = graph.get_tensor_by_name(\"b_fc2_tf:0\")\n",
    "        \n",
    "        # activation tensors\n",
    "        self.h_conv1_tf = graph.get_tensor_by_name('h_conv1_tf:0')  \n",
    "        self.h_pool1_tf = graph.get_tensor_by_name('h_pool1_tf:0')\n",
    "        self.h_conv2_tf = graph.get_tensor_by_name('h_conv2_tf:0')\n",
    "        self.h_pool2_tf = graph.get_tensor_by_name('h_pool2_tf:0')\n",
    "        self.h_conv3_tf = graph.get_tensor_by_name('h_conv3_tf:0')\n",
    "        self.h_pool3_tf = graph.get_tensor_by_name('h_pool3_tf:0')\n",
    "        self.h_fc1_tf = graph.get_tensor_by_name('h_fc1_tf:0')\n",
    "        self.z_pred_tf = graph.get_tensor_by_name('z_pred_tf:0')\n",
    "        \n",
    "        # training and prediction tensors\n",
    "        self.learn_rate_tf = graph.get_tensor_by_name(\"learn_rate_tf:0\")\n",
    "        self.keep_prob_tf = graph.get_tensor_by_name(\"keep_prob_tf:0\")\n",
    "        self.cross_entropy_tf = graph.get_tensor_by_name('cross_entropy_tf:0')\n",
    "        self.train_step_tf = graph.get_operation_by_name('train_step_tf')\n",
    "        self.z_pred_tf = graph.get_tensor_by_name('z_pred_tf:0')\n",
    "        self.y_pred_proba_tf = graph.get_tensor_by_name(\"y_pred_proba_tf:0\")\n",
    "        self.y_pred_correct_tf = graph.get_tensor_by_name('y_pred_correct_tf:0')\n",
    "        self.accuracy_tf = graph.get_tensor_by_name('accuracy_tf:0')\n",
    "        \n",
    "        # tensor of stored losses and accuracies during training\n",
    "        self.train_loss_tf = graph.get_tensor_by_name(\"train_loss_tf:0\")\n",
    "        self.train_acc_tf = graph.get_tensor_by_name(\"train_acc_tf:0\")\n",
    "        self.valid_loss_tf = graph.get_tensor_by_name(\"valid_loss_tf:0\")\n",
    "        self.valid_acc_tf = graph.get_tensor_by_name(\"valid_acc_tf:0\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    def attach_summary(self, sess):\n",
    "        \"\"\"\n",
    "        Create summary tensors for tensorboard.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            sess - the session for which we want to create summaries\n",
    "        Returns:\n",
    "            None\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        self.summary_variable(self.W_conv1_tf, 'W_conv1_tf')\n",
    "        self.summary_variable(self.b_conv1_tf, 'b_conv1_tf')\n",
    "        self.summary_variable(self.W_conv2_tf, 'W_conv2_tf')\n",
    "        self.summary_variable(self.b_conv2_tf, 'b_conv2_tf')\n",
    "        self.summary_variable(self.W_conv3_tf, 'W_conv3_tf')\n",
    "        self.summary_variable(self.b_conv3_tf, 'b_conv3_tf')\n",
    "        self.summary_variable(self.W_fc1_tf, 'W_fc1_tf')\n",
    "        self.summary_variable(self.b_fc1_tf, 'b_fc1_tf')\n",
    "        self.summary_variable(self.W_fc2_tf, 'W_fc2_tf')\n",
    "        self.summary_variable(self.b_fc2_tf, 'b_fc2_tf')\n",
    "        tf.summary.scalar('cross_entropy_tf', self.cross_entropy_tf)\n",
    "        tf.summary.scalar('accuracy_tf', self.accuracy_tf)\n",
    "\n",
    "        # merge all summaries for tensorboard\n",
    "        self.merged = tf.summary.merge_all()\n",
    "\n",
    "        # initialize summary writer \n",
    "        timestamp = datetime.datetime.now().strftime('%d-%m-%Y_%H-%M-%S')\n",
    "        filepath = os.path.join(os.getcwd(), self.base_dir, self.model_name, 'logs', (self.model_name+'_'+timestamp))\n",
    "        self.train_writer = tf.summary.FileWriter(os.path.join(filepath,'train'), sess.graph)\n",
    "        self.valid_writer = tf.summary.FileWriter(os.path.join(filepath,'valid'), sess.graph)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def network(self, X):\n",
    "        \"\"\"\n",
    "        Construct network architecture\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            X       (tensor) - input data with shape of (batch size; height of image; width of image ; num channels)\n",
    "        Returns:\n",
    "            Last layer of the network (for continuation)\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "\n",
    "        # 1.layer: convolution + max pooling\n",
    "        self.W_conv1_tf = self.weight_variable([self.s_f_conv1, self.s_f_conv1, 1, self.n_f_conv1], name = 'W_conv1_tf') # (3,3,1,36)\n",
    "        self.b_conv1_tf = self.bias_variable([self.n_f_conv1], name = 'b_conv1_tf') # (36)\n",
    "        self.h_conv1_tf = tf.nn.relu(self.conv2d(X, self.W_conv1_tf) + self.b_conv1_tf, name = 'h_conv1_tf') # (.,28,28,36)\n",
    "        self.h_pool1_tf = self.max_pool_2x2(self.h_conv1_tf, name = 'h_pool1_tf') # (.,14,14,36)\n",
    "\n",
    "        # 2.layer: convolution + max pooling\n",
    "        self.W_conv2_tf = self.weight_variable([self.s_f_conv2, self.s_f_conv2, self.n_f_conv1, self.n_f_conv2], name = 'W_conv2_tf')\n",
    "        self.b_conv2_tf = self.bias_variable([self.n_f_conv2], name = 'b_conv2_tf')\n",
    "        self.h_conv2_tf = tf.nn.relu(self.conv2d(self.h_pool1_tf, self.W_conv2_tf) + self.b_conv2_tf, name ='h_conv2_tf') #(.,14,14,36)\n",
    "        self.h_pool2_tf = self.max_pool_2x2(self.h_conv2_tf, name = 'h_pool2_tf') #(.,7,7,36)\n",
    "\n",
    "        # 3.layer: convolution + max pooling\n",
    "        self.W_conv3_tf = self.weight_variable([self.s_f_conv3, self.s_f_conv3, self.n_f_conv2, self.n_f_conv3], name = 'W_conv3_tf')\n",
    "        self.b_conv3_tf = self.bias_variable([self.n_f_conv3], name = 'b_conv3_tf')\n",
    "        self.h_conv3_tf = tf.nn.relu(self.conv2d(self.h_pool2_tf, self.W_conv3_tf) + self.b_conv3_tf, name = 'h_conv3_tf') #(.,7,7,36)\n",
    "        self.h_pool3_tf = self.max_pool_2x2(self.h_conv3_tf, name = 'h_pool3_tf') # (.,4,4,36)\n",
    "\n",
    "        # 4.layer: fully connected\n",
    "        self.W_fc1_tf = self.weight_variable([4*4*self.n_f_conv3,self.n_n_fc1], name = 'W_fc1_tf') # (4*4*36, 576)\n",
    "        self.b_fc1_tf = self.bias_variable([self.n_n_fc1], name = 'b_fc1_tf') # (576)\n",
    "        self.h_pool3_flat_tf = tf.reshape(self.h_pool3_tf, [-1,4*4*self.n_f_conv3], name = 'h_pool3_flat_tf') # (.,576)\n",
    "        self.h_fc1_tf = tf.nn.relu(tf.matmul(self.h_pool3_flat_tf, self.W_fc1_tf) + self.b_fc1_tf, name = 'h_fc1_tf') # (.,576)\n",
    "      \n",
    "        # add dropout\n",
    "        self.keep_prob_tf = tf.placeholder(dtype=tf.float32, name = 'keep_prob_tf')\n",
    "        self.h_fc1_drop_tf = tf.nn.dropout(self.h_fc1_tf, self.keep_prob_tf, name = 'h_fc1_drop_tf')\n",
    "\n",
    "        # 5.layer: fully connected\n",
    "        self.W_fc2_tf = self.weight_variable([self.n_n_fc1, 10], name = 'W_fc2_tf')\n",
    "        self.b_fc2_tf = self.bias_variable([10], name = 'b_fc2_tf')\n",
    "        \n",
    "        self.z_pred_tf = tf.add(tf.matmul(self.h_fc1_drop_tf, self.W_fc2_tf), self.b_fc2_tf, name = 'z_pred_tf')# => (.,10)\n",
    "\n",
    "        return self.z_pred_tf\n",
    "\n",
    "    def metrics(self, Y, Y_pred):\n",
    "        \"\"\"\n",
    "        Some metric, here I use simple accuracy\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            Y       (array_like) - actual labels\n",
    "            Y_pred  (array_like) - predicted labels\n",
    "        Returns:\n",
    "            Float (Accuracy)\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "\n",
    "        Y = Y.reshape(-1,)\n",
    "        Y_pred = Y_pred.reshape(-1,)\n",
    "        return np.mean(Y == Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from data_loader import *\n",
    "from utils import *\n",
    "from abc import abstractmethod\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os;\n",
    "import datetime  \n",
    "import cv2\n",
    "\n",
    "class BaseNN:\n",
    "    def __init__(self, train_images_dir, val_images_dir, test_images_dir, num_epochs, train_batch_size,\n",
    "                 val_batch_size, test_batch_size, height_of_image, width_of_image, num_channels, \n",
    "                 num_classes, learning_rate, base_dir, max_to_keep, model_name, keep_prob):\n",
    "\n",
    "        self.data_loader = DataLoader(train_images_dir, val_images_dir, test_images_dir, train_batch_size, \n",
    "                val_batch_size, test_batch_size, height_of_image, width_of_image, num_channels, num_classes)\n",
    "\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.base_dir = base_dir\n",
    "        self.max_to_keep = max_to_keep\n",
    "        self.model_name = model_name\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        self.n_log_step = 0 # counting current number of mini batches trained on\n",
    "    \n",
    "    def create_network(self):\n",
    "        \"\"\"\n",
    "        Create base components of the network.\n",
    "        Main structure of network will be described in network function.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            None\n",
    "        Returns:\n",
    "            None\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # variables for input and output \n",
    "        self.x_data_tf = tf.placeholder(dtype=tf.float32, shape=[None, self.data_loader.height_of_image, self.data_loader.width_of_image, self.data_loader.num_channels], name='x_data_tf')\n",
    "        self.y_data_tf = tf.placeholder(dtype=tf.float32, shape=[None, self.data_loader.num_classes], name='y_data_tf')\n",
    "\n",
    "        self.z_pred_tf = self.network(self.x_data_tf)\n",
    "\n",
    "        # cost function\n",
    "        self.cross_entropy_tf = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=self.y_data_tf, logits=self.z_pred_tf), name = 'cross_entropy_tf')\n",
    "\n",
    "        # optimisation function\n",
    "        self.learn_rate_tf = tf.placeholder(dtype=tf.float32, name=\"learn_rate_tf\")\n",
    "        self.train_step_tf = tf.train.AdamOptimizer(self.learn_rate_tf).minimize(\n",
    "            self.cross_entropy_tf, name = 'train_step_tf')\n",
    "\n",
    "        # predicted probabilities in one-hot encoding\n",
    "        self.y_pred_proba_tf = tf.nn.softmax(self.z_pred_tf, name='y_pred_proba_tf') \n",
    "        \n",
    "        # tensor of correct predictions\n",
    "        self.y_pred_correct_tf = tf.equal(tf.argmax(self.y_pred_proba_tf, 1),\n",
    "                                          tf.argmax(self.y_data_tf, 1),\n",
    "                                          name = 'y_pred_correct_tf')  \n",
    "        \n",
    "        # accuracy \n",
    "        self.accuracy_tf = tf.reduce_mean(tf.cast(self.y_pred_correct_tf, dtype=tf.float32),\n",
    "                                         name = 'accuracy_tf')\n",
    "\n",
    "        # tensors to save intermediate accuracies and losses during training\n",
    "        self.train_loss_tf = tf.Variable(np.array([]), dtype=tf.float32, \n",
    "                                         name='train_loss_tf', validate_shape = False)\n",
    "        self.valid_loss_tf = tf.Variable(np.array([]), dtype=tf.float32,\n",
    "                                         name='valid_loss_tf', validate_shape = False)\n",
    "        self.train_acc_tf = tf.Variable(np.array([]), dtype=tf.float32, \n",
    "                                        name='train_acc_tf', validate_shape = False)\n",
    "        self.valid_acc_tf = tf.Variable(np.array([]), dtype=tf.float32, \n",
    "                                        name='valid_acc_tf', validate_shape = False)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def close_writers(self):\n",
    "        \"\"\"\n",
    "        Close train and validation summary writer.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            sess - the session we want to save\n",
    "        Returns:\n",
    "            None\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        self.train_writer.close()\n",
    "        self.valid_writer.close()\n",
    "\n",
    "        return None\n",
    "\n",
    "    def save_model(self, sess):\n",
    "        \"\"\"\n",
    "        Save tensors/summaries\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            sess - the session we want to save\n",
    "        Returns:\n",
    "            None\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(os.getcwd(), self.base_dir, self.model_name, 'checkpoints', self.model_name)\n",
    "        self.saver_tf.save(sess, filepath)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def train_model_helper(self, sess, n_epoch = 1):\n",
    "        \"\"\"\n",
    "        Helper function to train the model.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            sess - current session\n",
    "            n_epoch (int)         - number of epochs\n",
    "        Returns:\n",
    "            None\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        \n",
    "        train_loss, train_acc, valid_loss, valid_acc = [],[],[],[]\n",
    "        \n",
    "        # start timer\n",
    "        start = datetime.datetime.now()\n",
    "        print(datetime.datetime.now().strftime('%d-%m-%Y %H:%M:%S'),': start training')\n",
    "        print('learnrate = ', self.learning_rate,', n_epoch = ', n_epoch,\n",
    "              ', mb_size = ', self.data_loader.train_batch_size)\n",
    "        \n",
    "        # looping over epochs\n",
    "        for e in range(1, n_epoch+1):\n",
    "            index = 0\n",
    "            # looping over mini batches\n",
    "            for i in range(1, int(np.ceil(self.data_loader.get_train_data_size() / self.data_loader.train_batch_size))+1):\n",
    "                x_batch, y_batch = self.data_loader.train_data_loader(index)\n",
    "                x_valid, y_valid = self.data_loader.val_data_loader(0, randomization = True)\n",
    "                \n",
    "                self.sess.run(self.train_step_tf,feed_dict={self.x_data_tf: x_batch,\n",
    "                                                            self.y_data_tf: y_batch,\n",
    "                                                            self.keep_prob_tf: self.keep_prob,\n",
    "                                                            self.learn_rate_tf: self.learning_rate})\n",
    "\n",
    "                feed_dict_valid = {self.x_data_tf: x_valid,\n",
    "                                   self.y_data_tf: y_valid,\n",
    "                                   self.keep_prob_tf: 1.0}\n",
    "\n",
    "                feed_dict_train = {self.x_data_tf: x_batch,\n",
    "                                    self.y_data_tf: y_batch,\n",
    "                                    self.keep_prob_tf: 1.0}\n",
    "                \n",
    "                # store losses and accuracies\n",
    "                if i%self.validation_step == 0:\n",
    "                    valid_loss.append(sess.run(self.cross_entropy_tf,\n",
    "                                               feed_dict = feed_dict_valid))\n",
    "                    valid_acc.append(self.accuracy_tf.eval(session = sess, \n",
    "                                                           feed_dict = feed_dict_valid))\n",
    "                    print('%.0f epoch, %.0f iteration: val loss = %.4f, val acc = %.4f'%(\n",
    "                        e, i, valid_loss[-1],valid_acc[-1]))\n",
    "\n",
    "                # summary for tensorboard\n",
    "                if i%self.summary_step == 0:\n",
    "                    self.n_log_step += 1 # for logging the results\n",
    "                    train_summary = sess.run(self.merged, feed_dict={self.x_data_tf: x_batch, \n",
    "                                                                    self.y_data_tf: y_batch, \n",
    "                                                                    self.keep_prob_tf: 1.0})\n",
    "                    valid_summary = sess.run(self.merged, feed_dict = feed_dict_valid)\n",
    "                    self.train_writer.add_summary(train_summary, self.n_log_step)\n",
    "                    self.valid_writer.add_summary(valid_summary, self.n_log_step)\n",
    "\n",
    "                if i%self.display_step == 0:\n",
    "                    train_loss.append(sess.run(self.cross_entropy_tf,\n",
    "                                               feed_dict = feed_dict_train))\n",
    "                    train_acc.append(self.accuracy_tf.eval(session = sess, \n",
    "                                                           feed_dict = feed_dict_train))\n",
    "                    print('%.0f epoch, %.0f iteration: train loss = %.4f, train acc = %.4f'%(\n",
    "                        e, i,  train_loss[-1],train_acc[-1]))\n",
    "\n",
    "                # save current model to disk\n",
    "                if i%self.checkpoint_step == 0:\n",
    "                    self.save_model(sess)\n",
    "                \n",
    "                index += self.data_loader.train_batch_size\n",
    "                \n",
    "        # concatenate losses and accuracies and assign to tensor variables\n",
    "        tl_c = np.concatenate([self.train_loss_tf.eval(session=sess), train_loss], axis = 0)\n",
    "        vl_c = np.concatenate([self.valid_loss_tf.eval(session=sess), valid_loss], axis = 0)\n",
    "        ta_c = np.concatenate([self.train_acc_tf.eval(session=sess), train_acc], axis = 0)\n",
    "        va_c = np.concatenate([self.valid_acc_tf.eval(session=sess), valid_acc], axis = 0)\n",
    "   \n",
    "        sess.run(tf.assign(self.train_loss_tf, tl_c, validate_shape = False))\n",
    "        sess.run(tf.assign(self.valid_loss_tf, vl_c , validate_shape = False))\n",
    "        sess.run(tf.assign(self.train_acc_tf, ta_c , validate_shape = False))\n",
    "        sess.run(tf.assign(self.valid_acc_tf, va_c , validate_shape = False))\n",
    "        \n",
    "        print('running time for training: ', datetime.datetime.now() - start)\n",
    "        return None\n",
    "    \n",
    "    def train_model(self, display_step, validation_step, checkpoint_step, summary_step):\n",
    "        \"\"\"\n",
    "        Main function for model training.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            display_step       (int)   - Number of steps we cycle through before displaying detailed progress\n",
    "            validation_step    (int)   - Number of steps we cycle through before validating the model\n",
    "            checkpoint_step    (int)   - Number of steps we cycle through before saving checkpoint\n",
    "            summary_step       (int)   - Number of steps we cycle through before saving summary\n",
    "        Returns:\n",
    "            None\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        self.display_step = display_step\n",
    "        self.validation_step = validation_step\n",
    "        self.checkpoint_step = checkpoint_step\n",
    "        self.summary_step = summary_step\n",
    "        \n",
    "        # self.x_valid, self.y_valid = self.data_loader.all_val_data_loader()\n",
    "\n",
    "        self.saver_tf = tf.train.Saver(max_to_keep = self.max_to_keep)\n",
    "\n",
    "        # attach summaries\n",
    "        self.attach_summary(self.sess)\n",
    "\n",
    "        # training on original data\n",
    "        self.train_model_helper(self.sess, n_epoch = self.num_epochs)\n",
    "\n",
    "        # save final model\n",
    "        self.save_model(self.sess)\n",
    "\n",
    "        self.close_writers()\n",
    "\n",
    "    def get_accuracy(self, sess):\n",
    "        \"\"\"\n",
    "        Get accuracies of training and validation sets.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            sess - session\n",
    "        Returns:\n",
    "            tuple (tuple of lists) train and validation accuracies\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        train_acc = self.train_acc_tf.eval(session = sess)\n",
    "        valid_acc = self.valid_acc_tf.eval(session = sess)\n",
    "        return train_acc, valid_acc\n",
    "\n",
    "    def get_loss(self, sess):\n",
    "        \"\"\"\n",
    "        Get losses of training and validation sets.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            sess - session\n",
    "        Returns:\n",
    "            tuple (tuple of lists) train and validation losses\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        train_loss = self.train_loss_tf.eval(session = sess)\n",
    "        valid_loss = self.valid_loss_tf.eval(session = sess)\n",
    "        return train_loss, valid_loss \n",
    "\n",
    "    def forward(self, sess, x_data):\n",
    "        \"\"\"\n",
    "        Forward prediction of current graph.\n",
    "        Will be used in test_model method.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            sess                 - actual session\n",
    "            x_data (matrix_like) - data for which we want to calculate predicted probabilities\n",
    "        Returns:\n",
    "            vector_like - predicted probabilities for input data\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        y_pred_proba = self.y_pred_proba_tf.eval(session = sess, \n",
    "                                                 feed_dict = {self.x_data_tf: x_data,\n",
    "                                                              self.keep_prob_tf: 1.0})\n",
    "        return y_pred_proba\n",
    "    \n",
    "    def load_session_from_file(self, filename):\n",
    "        \"\"\"\n",
    "        Load session from file, restore graph, and load tensors.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            filename (string) - the name of the model (name of file we saved in disk)\n",
    "        Returns:\n",
    "            session\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        filepath = os.path.join(os.getcwd(), filename + '.meta')\n",
    "        # filepath = os.path.join(os.getcwd(), self.base_dir, filename + '.meta')\n",
    "        # filepath = os.path.join(os.getcwd(), self.base_dir, self.model_name, filename + '.meta')\n",
    "        # filepath = os.path.join(os.getcwd(), self.base_dir, self.model_name, 'checkpoints', filename + '.meta')\n",
    "        print(filepath)\n",
    "        saver = tf.train.import_meta_graph(filepath)\n",
    "        sess = tf.Session()\n",
    "        saver.restore(sess, self.model_name)\n",
    "        graph = tf.get_default_graph()\n",
    "        \n",
    "        self.load_tensors(graph)\n",
    "        \n",
    "        return sess\n",
    "\n",
    "    def test_model(self):\n",
    "        \"\"\"\n",
    "        load model and test on test data.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            None\n",
    "        Returns:\n",
    "            metric, defined in dnn class (for example accuracy)\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        x_test, y_test = self.data_loader.all_test_data_loader()\n",
    "                \n",
    "        sess = self.load_session_from_file(self.model_name)\n",
    "        \n",
    "        y_test_pred = {self.model_name : []}\n",
    "        y_test_pred_labels = {}\n",
    "        \n",
    "        accuracies = []\n",
    "        \n",
    "        # looping over mini batches\n",
    "        index = 0\n",
    "        for i in range(1, int(np.ceil(self.data_loader.get_test_data_size() / self.data_loader.test_batch_size))+1):\n",
    "            x_test, y_test = self.data_loader.test_data_loader(index)\n",
    "            y_test_pred[self.model_name].append(self.forward(sess, x_test))\n",
    "#             y_test_pred[self.model_name] = self.forward(sess, x_test)\n",
    "            \n",
    "            y_test_pred_labels[self.model_name] = one_hot_to_dense(y_test_pred[self.model_name])\n",
    "            y_test = one_hot_to_dense(y_test)\n",
    "            \n",
    "            accuracies.append(self.metrics(y_test, y_test_pred_labels[self.model_name]))\n",
    "        \n",
    "#         for i in y_test_pred.values():\n",
    "#             y_test_pred_ = np.concatenate( i, axis=0 )\n",
    "        \n",
    "        sess.close()\n",
    "        \n",
    "        return accuracies\n",
    "        \n",
    "#         return y_test_pred_\n",
    "    \n",
    "#         y_test_pred_labels[self.model_name] = one_hot_to_dense(y_test_pred[self.model_name])\n",
    "#         y_test = one_hot_to_dense(y_test)\n",
    "        \n",
    "#         print('Test Accuracy: ', self.metrics(y_test, y_test_pred_labels[self.model_name]))\n",
    "#         return self.metrics(y_test, y_test_pred_labels[self.model_name])\n",
    "\n",
    "    def initialize_network(self):\n",
    "        \"\"\"\n",
    "        Initialize network from last checkpoint if exists, otherwise initialize with random values.\n",
    "        -----------------\n",
    "        Parameters:\n",
    "            None\n",
    "        Returns:\n",
    "            metric, defined in dnn class (for example accuracy)\n",
    "        -----------------\n",
    "        \"\"\"\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        filepath = os.path.join(os.getcwd(), self.base_dir, self.model_name, 'checkpoints', self.model_name + '.meta')\n",
    "        if ~os.path.isdir(filepath):\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "        else:\n",
    "            self.sess = self.load_session_from_file(self.model_name)\n",
    "        return None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def network(self, X):\n",
    "        raise NotImplementedError('subclasses must override network()!')\n",
    "\n",
    "    @abstractmethod\n",
    "    def metrics(self, Y, y_pred):\n",
    "        raise NotImplementedError('subclasses must override metrics()!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_graph = DNN(\n",
    "    train_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/train/image/',\n",
    "    val_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/validation/image/',\n",
    "    test_images_dir='C:/_Files/MyProjects/ASDS_3/ASDS_DL/Homeworks/3_mnist/_inputs_image/all/test/image/',\n",
    "    num_epochs=2,\n",
    "    train_batch_size=100,\n",
    "    val_batch_size=100,\n",
    "    test_batch_size=1000,\n",
    "    height_of_image=28,\n",
    "    width_of_image=28,\n",
    "    num_channels=1,\n",
    "    num_classes=10,\n",
    "    learning_rate = 0.001,\n",
    "    base_dir='results',\n",
    "    max_to_keep=5,\n",
    "    model_name='nn_1',\n",
    "    keep_prob=0.33\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1106 15:23:17.058592  6436 deprecation.py:506] From <ipython-input-5-ece43f5a3479>:223: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1106 15:23:17.076544  6436 deprecation.py:323] From <ipython-input-1-2032920d05de>:49: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_graph.create_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_graph.initialize_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_step, validation_step, checkpoint_step, summary_step\n",
    "# nn_graph.train_model(100, 100, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1106 15:23:22.018013  6436 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\_Files\\MyProjects\\ASDS_3\\ASDS_DL\\Homeworks\\3_mnist\\Junk Codes\\nn_1.meta\n"
     ]
    }
   ],
   "source": [
    "ddxk = nn_graph.test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddxk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,],[4, 5, 6]])\n",
    "b = np.array([[1,2,3],[4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.concatenate((a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# for lists\n",
    "d = defaultdict(list)\n",
    "d['C1'].append([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['C1'].append([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3]])\n",
    "d = {'index': []}\n",
    "d['index'].extend(list(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['index'].extend(list(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3], [1,2,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "from tensorflow.contrib.layers.python.layers import layers\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import rnn_cell_impl\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "# from . import resnet_model\n",
    "# from .inception.slim import inception_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_settings(label_count, sample_rate, clip_duration_ms,\n",
    "                           window_size_ms, window_stride_ms,\n",
    "                           dct_coefficient_count,use_spectrogram=False):\n",
    "    \"\"\"Calculates common settings needed for all models.\n",
    "  \n",
    "    Args:\n",
    "      label_count: How many classes are to be recognized.\n",
    "      sample_rate: Number of audio samples per second.\n",
    "      clip_duration_ms: Length of each audio clip to be analyzed.\n",
    "      window_size_ms: Duration of frequency analysis window.\n",
    "      window_stride_ms: How far to move in time between frequency windows.\n",
    "      dct_coefficient_count: Number of frequency bins to use for analysis.\n",
    "  \n",
    "    Returns:\n",
    "      Dictionary containing common settings.\n",
    "    \"\"\"\n",
    "    desired_samples = int(sample_rate * clip_duration_ms / 1000)\n",
    "    window_size_samples = int(sample_rate * window_size_ms / 1000)\n",
    "    window_stride_samples = int(sample_rate * window_stride_ms / 1000)\n",
    "    length_minus_window = (desired_samples - window_size_samples)\n",
    "    if length_minus_window < 0:\n",
    "        spectrogram_length = 0\n",
    "    else:\n",
    "        spectrogram_length = 1 + int(length_minus_window / window_stride_samples)\n",
    "    fingerprint_size = dct_coefficient_count * spectrogram_length\n",
    "    return {\n",
    "        'desired_samples': desired_samples,\n",
    "        'window_size_samples': window_size_samples,\n",
    "        'window_stride_samples': window_stride_samples,\n",
    "        'spectrogram_length': spectrogram_length,\n",
    "        'dct_coefficient_count': dct_coefficient_count,\n",
    "        'fingerprint_size': fingerprint_size,\n",
    "        'label_count': label_count,\n",
    "        'sample_rate': sample_rate,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_model(fingerprint_input, model_settings, model_architecture,\n",
    "                 is_training, runtime_settings=None,model_size_info=None):\n",
    "    \"\"\"Builds a model of the requested architecture compatible with the settings.\n",
    "  \n",
    "    There are many possible ways of deriving predictions from a spectrogram\n",
    "    input, so this function provides an abstract interface for creating different\n",
    "    kinds of models in a black-box way. You need to pass in a TensorFlow node as\n",
    "    the 'fingerprint' input, and this should output a batch of 1D features that\n",
    "    describe the audio. Typically this will be derived from a spectrogram that's\n",
    "    been run through an MFCC, but in theory it can be any feature vector of the\n",
    "    size specified in model_settings['fingerprint_size'].\n",
    "  \n",
    "    The function will build the graph it needs in the current TensorFlow graph,\n",
    "    and return the tensorflow output that will contain the 'logits' input to the\n",
    "    softmax prediction process. If training flag is on, it will also return a\n",
    "    placeholder node that can be used to control the dropout amount.\n",
    "  \n",
    "    See the implementations below for the possible model architectures that can be\n",
    "    requested.\n",
    "  \n",
    "    Args:\n",
    "      fingerprint_input: TensorFlow node that will output audio feature vectors.\n",
    "      model_settings: Dictionary of information about the model.\n",
    "      model_architecture: String specifying which kind of model to create.\n",
    "      is_training: Whether the model is going to be used for training.\n",
    "      runtime_settings: Dictionary of information about the runtime.\n",
    "  \n",
    "    Returns:\n",
    "      TensorFlow node outputting logits results, and optionally a dropout\n",
    "      placeholder.\n",
    "  \n",
    "    Raises:\n",
    "      Exception: If the architecture type isn't recognized.\n",
    "    \"\"\"\n",
    "    if model_architecture == 'single_fc':\n",
    "        return create_single_fc_model(fingerprint_input, model_settings,\n",
    "                                      is_training)\n",
    "    elif model_architecture == 'conv':\n",
    "        return create_conv_model(fingerprint_input, model_settings, is_training)\n",
    "    elif model_architecture == 'low_latency_conv':\n",
    "        return create_low_latency_conv_model(fingerprint_input, model_settings,\n",
    "                                             is_training)\n",
    "    elif model_architecture == 'low_latency_svdf':\n",
    "        return create_low_latency_svdf_model(fingerprint_input, model_settings,\n",
    "                                             is_training, runtime_settings)\n",
    "    elif model_architecture == 'convlstm':\n",
    "        return create_multilayer_convlstm_model(fingerprint_input, model_settings,\n",
    "                                                is_training)\n",
    "    elif model_architecture == 'lstm_l':\n",
    "        return create_lstm_l_model(fingerprint_input,model_settings,is_training)\n",
    "    elif model_architecture == 'ds_cnn':\n",
    "        return create_ds_cnn_model(fingerprint_input,model_settings,model_size_info,is_training)\n",
    "    elif model_architecture == 'ds_cnn_spec':\n",
    "        return create_ds_cnn_model(fingerprint_input,model_settings,model_size_info,is_training)\n",
    "    elif model_architecture == 'inception':\n",
    "        return create_inception_model(fingerprint_input,model_settings,is_training)\n",
    "    elif model_architecture == 'c_rnn':\n",
    "        return create_crnn_model(fingerprint_input,model_settings,model_size_info,is_training)\n",
    "    elif model_architecture=='c_rnn_spec':\n",
    "        return create_crnn_model(fingerprint_input,model_settings,model_size_info,is_training)\n",
    "    elif model_architecture=='gru':\n",
    "        return create_gru_model(fingerprint_input,model_settings,model_size_info,is_training)\n",
    "    else:\n",
    "        raise Exception('model_architecture argument \"' + model_architecture +\n",
    "                        '\" not recognized, should be one of \"single_fc\", \"conv\",' +\n",
    "                        ' \"low_latency_conv, or \"low_latency_svdf\"')\n",
    "\n",
    "\n",
    "def load_variables_from_checkpoint(sess, start_checkpoint):\n",
    "    \"\"\"Utility function to centralize checkpoint restoration.\n",
    "  \n",
    "    Args:\n",
    "      sess: TensorFlow session.\n",
    "      start_checkpoint: Path to saved checkpoint on disk.\n",
    "    \"\"\"\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    saver.restore(sess, start_checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
